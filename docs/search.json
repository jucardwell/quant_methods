[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quant_methods",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "2  Schedule",
    "section": "",
    "text": "Date\nTopic\nReading\nDue\n\n\n\n\n2026-01-07\nIntroduction to Class\n\n\n\n\n2026-01-09\nClass Contract\n\n\n\n\n2026-01-12\nHow do we think about space?\n\n\n\n\n2026-01-14\nThe Quantitative Revolution\n\n\n\n\n2026-01-16\nEarly Laws\n\n\n\n\n2026-01-19\nNO CLASS\n\n\n\n\n2026-01-21\nThe Problem with Space\n\n\n\n\n2026-01-23\nThe Problem with Space\n\nLab #1\n\n\n2026-01-26\nDescribing Data\n\n\n\n\n2026-01-28\nDescribing Data\n\n\n\n\n2026-01-30\nDescribing Data\n\nLab #2\n\n\n2026-02-02\nDescribing Spatial Data\n\n\n\n\n2026-02-04\nDescribing Spatial Data\n\n\n\n\n2026-02-06\nDescribing Spatial Data\n\n\n\n\n2026-02-09\nNO CLASS\n\n\n\n\n2026-02-11\nCase Study #1\n\n\n\n\n2026-02-13\nWhat is probability?\n\nLab #3\n\n\n2026-02-16\nProbability Mapping\n\n\n\n\n2026-02-18\nHow do we take a sample?\n\n\n\n\n2026-02-20\nDiscrete Probability Distributions\n\n\n\n\n2026-02-23\nDiscrete Probability Distributions\n\n\n\n\n2026-02-25\nContinuous Probability Distributions\n\n\n\n\n2026-02-27\nContinuous Probability Distributions\n\n\n\n\n2026-03-02\nGlobal Autocorrelation\n\n\n\n\n2026-03-04\nLocal Autocorrelation\n\n\n\n\n2026-03-06\nPoint Pattern Analysis\n\nLab #4\n\n\n2026-03-09\nFLEX\n\n\n\n\n2026-03-11\nCase Study #2\n\n\n\n\n2026-03-13\nFLEX\n\nLab #5\n\n\n2026-03-16\nNO CLASS\n\n\n\n\n2026-03-18\nNO CLASS\n\n\n\n\n2026-03-20\nNO CLASS\n\n\n\n\n2026-03-23\nForming Hypotheses\n\n\n\n\n2026-03-25\nSpatial is Special (natural samples)\n\n\n\n\n2026-03-27\nHypothesis Testing #1\n\n\n\n\n2026-03-30\nHypothesis Testing #2\n\n\n\n\n2026-04-01\nHypothesis Testing #3\n\n\n\n\n2026-04-03\nNO CLASS\n\n\n\n\n2026-04-06\nHypothesis Testing #4\n\nLab #6A\n\n\n2026-04-08\nHypothesis Testing #5\n\n\n\n\n2026-04-10\nPick the Test\n\n\n\n\n2026-04-13\nCorrelation\n\n\n\n\n2026-04-15\nSpatial Correlation\n\n\n\n\n2026-04-17\nSpatial Correlation\n\nLab #6B\n\n\n2026-04-20\nBringing it Together\n\n\n\n\n2026-04-22\n\n\n\n\n\n2026-04-24\n\nLab #7\n\n\n\n2026-04-27\nCase Study #3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Schedule</span>"
    ]
  },
  {
    "objectID": "competencies.html",
    "href": "competencies.html",
    "title": "3  Course Competencies",
    "section": "",
    "text": "3.1 Tier 1: Foundations of Quantitative Methods",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Competencies</span>"
    ]
  },
  {
    "objectID": "basic_R.html",
    "href": "basic_R.html",
    "title": "4  Working in R and R Studio",
    "section": "",
    "text": "4.1 Downloading and Installing R and RStudio\nThis is NOT a coding class, but we will be using R for our data analysis. Because of this, while I don’t expect you to be able to generate code independently, I do expect you to develop a competency in navigating the RStudio environment and modifying code that I give you to complete specific tasks. This chapter will introduce you to R and RStudio.\nRStudio is an Integrated Development Environment (IDE) for R. RStudio provides a user-friendly interface for working with R code. R is an open-source programming language that has excellent capabilities for analyzing and visualizing spatial data.\nTo use R and RStudio, you will need to download and install both programs on your computer.\nOnce you have downloaded and installed RStudio, open up the application. We need to change one global setting before we start. You only need to do this once! Navigate Tools &gt; Global Options, unclick “Restore .RData into Workspace at Startup” and choosing Never on the “Save workspace to .RData on exit”.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#downloading-and-installing-r-and-rstudio",
    "href": "basic_R.html#downloading-and-installing-r-and-rstudio",
    "title": "4  Working in R and R Studio",
    "section": "",
    "text": "Download R (make sure to download the correct version for your operating system)\nDownload RStudio",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#the-rstudio-environment",
    "href": "basic_R.html#the-rstudio-environment",
    "title": "4  Working in R and R Studio",
    "section": "4.2 The RStudio Environment",
    "text": "4.2 The RStudio Environment\nThe RStudio environment consists of four panels.\n\nSource: This is where you can edit R files (either .R or .Rmd) and run R code.\nConsole: The console is an interactive R environment, where you can easily run R commands. Commands written in the console are not saved into any file. It can be good to use the console for quickly testing R code\nEnvironment: This is where R objects made during each session will appear.\nOutput: This is where charts and graphs will appear.\n\n\n\n\nRStudio Panes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#creating-and-saving-files",
    "href": "basic_R.html#creating-and-saving-files",
    "title": "4  Working in R and R Studio",
    "section": "4.3 Creating and Saving Files",
    "text": "4.3 Creating and Saving Files\nIn this class, we will primarily be working with .Rmd files. An R Markdown or .Rmd is a document format that is designed to easily integrate text and code. This means that you can easily type answers to questions, descriptions of code outputs, and other assignment tasks within the same document as your code.\nTo make a new .Rmd file, navigate to File &gt; New Script &gt; R Markdown\nI will sometimes provide you a .Rmd template for in-class exercises. You should save these files into a GEOG391 folder on your computer (not in the downloads folder please!)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#downloading-packages",
    "href": "basic_R.html#downloading-packages",
    "title": "4  Working in R and R Studio",
    "section": "4.4 Downloading Packages",
    "text": "4.4 Downloading Packages\nR packages extend the functionality of Base R and must be downloaded. We will use a small set of libraries in this class.\n\ntmap: This is the main mapping package we will use\ngt: We will use this package to create nice tables\ntidyverse: This is the package we will use to create “tidy” analysis workflows. Tidyverse also includes ggplot2 which will be our main package to create graphics\nsf: This package allows us to read and manipulate spatial datasets\n\nYou should copy and paste each of these commands into the Console one by one to download the packages.\n\ninstall.packages(\"tmap\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"gt\")\ninstall.packages(\"sf\")\n\nOnce you have downloaded the packages, you will need to load them every time you want to use them. I will always tell you what packages to load!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#basic-.rmd-functionality",
    "href": "basic_R.html#basic-.rmd-functionality",
    "title": "4  Working in R and R Studio",
    "section": "5.1 Basic .Rmd Functionality",
    "text": "5.1 Basic .Rmd Functionality\nWelcome to your first .Rmd! Each .Rmd starts with a header, where you can change the name and the date.\nA .Rmd is made up of two main components: code chunks (this is where you write code) and text (any words that are outside of a code chunk). Any time you are writing code, it needs to be in a code chunk. Chunks can then be run to execute the code within the chunk. The idea of chunking is to group code that accomplishes a single, related task into one code chunk. When you begin a new step or process in your workflow, you start a new chunk.\nYour .Rmd template already has a header (where you can change the name and date) and two code chunks. The first code chunk is the SET UP CHUNK. The second code chunk is the LIBRARY AND DATA CHUNK. To load the libraries and data, you should RUN CODE CHUNK.\n\nIf everything is working, ‘parks’ should appear in your environment. The ‘parks’ dataset is now an “object”.\nWe can view objects by double clicking the object in the environment. A data table should appear with all the information about the parks.\n\nAdd a new code chunk and add the code below to create your first map. Every command that you create should have a comment (#) above that explains what that command is doing. If you run that chunk, a map should appear below.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#knitting-a-.rmd",
    "href": "basic_R.html#knitting-a-.rmd",
    "title": "4  Working in R and R Studio",
    "section": "5.2 Knitting a .Rmd",
    "text": "5.2 Knitting a .Rmd\nKnitting a .Rmd means that you create a rendered .html including your code, output, and text. To knit your document, click the knit button. It will create a .html in whatever folder you have the .Rmd saved in (hopefully your GEOG391 folder!)\n\nWhen your knitted document opens, it should look something like this",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "basic_R.html#adding-text-and-formatting-a-.rmd",
    "href": "basic_R.html#adding-text-and-formatting-a-.rmd",
    "title": "4  Working in R and R Studio",
    "section": "5.3 Adding Text and Formatting a .Rmd",
    "text": "5.3 Adding Text and Formatting a .Rmd\nThe great thing about working in .Rmds is that you can create nicely formatted documents that merge code and text. At the bottom of the .Rmd is some tips for formatting nice-looking documents. Play around with adding some heading and text to your document.\nSometimes we want to modify what code and output actually appears in our knitted document. The commands at the bottom of the document explain which chunk modifiers to. See if you can figure out how to change the map chunk to not include the code in the knitted document.\nAfter you are done working in your .Rmd, make sure to save it!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working in R and R Studio</span>"
    ]
  },
  {
    "objectID": "describing_data.html",
    "href": "describing_data.html",
    "title": "6  Describing Data",
    "section": "",
    "text": "6.1 Orienting Ourselves to the Datasets\nIn this chapter, we will apply what we know about describing data to explore three datasets. Descriptive statistics are a set of tools that allow us to summarize a given dataset based on various characteristics.\nWe will use the following spatial data:\nThe last two datasets are aggregated summaries rather than raw measurements. The Climate Normals are temporal averages,meaning that the data represent the average conditions over a 30-year period. The ACS data are spatial averages, which summarize individual responses within defined geographic areas. This is a requirement for many spatial datasets, even when individual-level data is available, to maintain anonymity.\nAs a result, when we analyze these datasets, we are working with information that has already been statistically summarized over time and space, and our own statistical descriptions will be second-order summaries that describe patterns in data that have already been aggregated.\nTo follow along with this tutorial, download the .Rmd template here. The template already includes a code chunk for loading libraries and reading in the data, along with labeled empty chunks for each section of the tutorial. As you work through the code examples below, add each set of commands to the chunk with the matching section heading in your template.\nThe ‘glimpse()’ function provides a quick overview of the structure and contents of a dataset. It displays the data types of each column and a preview of the data, making it easier to understand the dataset at a glance.\n## glimpse at the rurality data\nglimpse(rurality)\n\nRows: 2,672\nColumns: 4\n$ GEOID           &lt;chr&gt; \"37141920300\", \"37141990100\", \"37071031600\", \"37071031…\n$ ruca_code       &lt;chr&gt; \"Metropolitan high commuting\", \"Not coded\", \"Metropoli…\n$ nc_rural_center &lt;chr&gt; \"Rural\", \"Rural\", \"Not Rural\", \"Not Rural\", \"Rural\", \"…\n$ geometry        &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-78.15648 3..., MULTIPOLY…\n\n## glimpse at the climate normals data\nglimpse(climate_normals)\n\nRows: 469,758\nColumns: 2\n$ summer_temp &lt;dbl&gt; 56.25781, 56.25312, 56.27656, 56.40313, 56.46875, 57.36875…\n$ geometry    &lt;POINT [°]&gt; POINT (-124.6875 48.0625), POINT (-124.6875 48.10417…\n\n#glimpse at acs data\nglimpse(acs_tract_nc)\n\nRows: 2,672\nColumns: 54\n$ STATEFP          &lt;chr&gt; \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\",…\n$ COUNTYFP         &lt;chr&gt; \"031\", \"031\", \"031\", \"031\", \"031\", \"031\", \"031\", \"119…\n$ TRACTCE          &lt;chr&gt; \"970402\", \"970502\", \"970805\", \"970903\", \"970101\", \"97…\n$ GEOID            &lt;chr&gt; \"37031970402\", \"37031970502\", \"37031970805\", \"3703197…\n$ NAME             &lt;chr&gt; \"9704.02\", \"9705.02\", \"9708.05\", \"9709.03\", \"9701.01\"…\n$ NAMELSAD         &lt;chr&gt; \"Census Tract 9704.02\", \"Census Tract 9705.02\", \"Cens…\n$ MTFCC            &lt;chr&gt; \"G5020\", \"G5020\", \"G5020\", \"G5020\", \"G5020\", \"G5020\",…\n$ FUNCSTAT         &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"…\n$ ALAND            &lt;dbl&gt; 2395384, 6094915, 139248627, 5864308, 28018671, 97430…\n$ AWATER           &lt;dbl&gt; 1553995, 4244211, 421209, 30151466, 35773987, 1568180…\n$ INTPTLAT         &lt;chr&gt; \"+34.7239933\", \"+34.7517653\", \"+34.7817392\", \"+34.701…\n$ INTPTLON         &lt;chr&gt; \"-076.7087383\", \"-076.7387561\", \"-077.0184797\", \"-076…\n$ tract_name       &lt;chr&gt; \"Census Tract 9704.02, Carteret County, North Carolin…\n$ total_pop        &lt;dbl&gt; 1309, 2177, 1694, 1211, 965, 4244, 2306, 2720, 6076, …\n$ median_age       &lt;dbl&gt; 39.3, 56.1, 43.3, 62.1, 48.4, 55.9, 42.3, 28.3, 29.7,…\n$ pct_white        &lt;dbl&gt; 64.17, 93.11, 85.36, 85.47, 86.22, 72.64, 72.94, 6.32…\n$ pct_black        &lt;dbl&gt; 21.01, 2.30, 0.00, 0.66, 3.21, 17.86, 9.71, 45.96, 30…\n$ pct_aian         &lt;dbl&gt; 0.15, 0.00, 0.41, 0.41, 0.00, 2.00, 0.00, 0.00, 0.05,…\n$ pct_asian        &lt;dbl&gt; 0.00, 0.00, 1.89, 0.00, 0.00, 0.00, 4.68, 2.90, 1.50,…\n$ pct_nhpi         &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.17, 1.32, 0.00,…\n$ pct_other_race   &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 1.06, 0.22, 0.00, 0.00,…\n$ pct_two_races    &lt;dbl&gt; 12.15, 0.32, 10.68, 12.80, 0.83, 3.79, 8.24, 1.69, 0.…\n$ pct_hisp         &lt;dbl&gt; 2.52, 4.27, 1.65, 0.66, 9.74, 2.64, 4.03, 41.80, 63.8…\n$ pct_family_hh    &lt;dbl&gt; 39.31, 64.53, 73.30, 64.78, 65.03, 64.45, 62.60, 61.2…\n$ married_fam      &lt;dbl&gt; 19.08, 55.75, 55.45, 61.55, 45.08, 55.44, 42.31, 28.3…\n$ male_hh          &lt;dbl&gt; 3.71, 0.00, 3.81, 0.48, 9.84, 3.64, 2.88, 9.40, 8.66,…\n$ female_hh        &lt;dbl&gt; 16.52, 8.77, 14.03, 2.75, 10.11, 5.37, 17.40, 23.56, …\n$ pct_nonfam_hh    &lt;dbl&gt; 60.69, 35.47, 26.70, 35.22, 34.97, 35.55, 37.40, 38.7…\n$ nonfam_male_hh   &lt;dbl&gt; 37.00, 16.13, 15.26, 14.22, 18.31, 17.25, 13.37, 13.7…\n$ nonfam_fem_hh    &lt;dbl&gt; 23.69, 19.34, 11.44, 21.00, 16.67, 18.30, 24.04, 25.0…\n$ av_hh_size       &lt;dbl&gt; 1.64, 2.05, 2.31, 1.96, 2.37, 2.03, 2.21, 3.01, 2.89,…\n$ pct_less_hs      &lt;dbl&gt; 17.40, 7.95, 9.49, 0.00, 8.64, 18.64, 6.50, 19.05, 29…\n$ pct_hs           &lt;dbl&gt; 52.21, 53.92, 69.15, 41.87, 67.83, 59.63, 61.34, 60.9…\n$ pc_bach          &lt;dbl&gt; 30.38, 38.13, 21.36, 58.13, 23.54, 21.73, 32.17, 19.9…\n$ pct_unemp        &lt;dbl&gt; 5.67, 0.41, 0.86, 1.47, 1.45, 0.13, 0.95, 5.91, 8.52,…\n$ med_hh_inc       &lt;dbl&gt; 45391, 86583, 66548, 96198, 58750, 67195, 63393, 4844…\n$ gini_index       &lt;dbl&gt; 0.4094, 0.4039, 0.4377, 0.4689, 0.5036, 0.4248, 0.462…\n$ pct_owner        &lt;dbl&gt; 41.87, 87.64, 81.47, 86.11, 81.42, 70.29, 62.98, 25.1…\n$ pct_renter       &lt;dbl&gt; 58.13, 12.36, 18.53, 13.89, 18.58, 29.71, 37.02, 74.8…\n$ pct_vacant       &lt;dbl&gt; 19.57, 10.09, 18.90, 78.84, 45.29, 14.68, 8.29, 6.22,…\n$ med_year_built   &lt;dbl&gt; 1962, 1985, 1997, 1988, 1965, 1994, 1984, 1983, 1986,…\n$ pct_gas          &lt;dbl&gt; 18.95, 13.02, 7.36, 13.41, 31.15, 2.30, 3.56, 49.89, …\n$ pct_electric     &lt;dbl&gt; 77.21, 85.66, 85.01, 84.81, 53.83, 96.26, 95.67, 49.7…\n$ med_house_val    &lt;dbl&gt; 234100, 329900, 208800, 669300, 168500, 262600, 23640…\n$ med_rent         &lt;dbl&gt; 781, 1241, 1133, 1489, 1000, 762, 1184, 1355, 1309, 2…\n$ pct_pov          &lt;dbl&gt; 26.02, 4.10, 9.63, 4.21, 16.63, 9.50, 18.35, 16.09, 1…\n$ pct_drive_work   &lt;dbl&gt; 90.34, 89.37, 87.40, 75.43, 78.46, 85.78, 94.20, 86.3…\n$ pct_public_trans &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 4.12, 3.60,…\n$ bike_walk        &lt;dbl&gt; 3.22, 0.00, 2.95, 0.00, 2.25, 0.00, 1.40, 2.22, 0.00,…\n$ pct_wfh          &lt;dbl&gt; 6.44, 7.95, 8.17, 24.57, 8.36, 10.30, 4.12, 5.43, 11.…\n$ av_commute       &lt;dbl&gt; 17, 20, 26, 28, 32, 17, 23, 19, 25, 24, 25, 23, 25, 2…\n$ pct_no_veh       &lt;dbl&gt; 27.53, 0.57, 0.00, 3.07, 4.92, 2.97, 3.94, 7.52, 7.90…\n$ no_health_insur  &lt;dbl&gt; 19.39, 7.21, 15.13, 4.29, 7.85, 16.02, 6.16, 14.72, 4…\n$ geometry         &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-76.74294 3..., MULTIPOL…",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#creating-a-basic-table-for-descriptive-statistics",
    "href": "describing_data.html#creating-a-basic-table-for-descriptive-statistics",
    "title": "6  Describing Data",
    "section": "7.1 Creating a Basic Table for Descriptive Statistics",
    "text": "7.1 Creating a Basic Table for Descriptive Statistics\nThe ‘gt’ package in R is a tool to create “presentation-ready” tables.\n\n## create a basic table of descriptive statistics for RUCA codes\nrurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(n = n())  |&gt;  gt() \n\n\n\n\n\n\n\nruca_code\nn\n\n\n\n\nMetropolitan core\n1436\n\n\nMetropolitan high commuting\n406\n\n\nMetropolitan low commuting\n87\n\n\nMicropolitan core\n218\n\n\nMicropolitan high commuting\n134\n\n\nMicropolitan low commuting\n79\n\n\nNot coded\n12\n\n\nRural area\n225\n\n\nSmall town core\n39\n\n\nSmall town high commuting\n20\n\n\nSmall town low commuting\n16\n\n\n\n\n\n\n\nQ3. What is the mode of the ruca_code?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#visualizing-descriptive-statistics",
    "href": "describing_data.html#visualizing-descriptive-statistics",
    "title": "6  Describing Data",
    "section": "7.2 Visualizing Descriptive Statistics",
    "text": "7.2 Visualizing Descriptive Statistics\nOur descriptive statistics table has given us some useful information about our dataset. Visualizing our data can help us understand our descriptive statistics better.\n\n7.2.1 Bar Chart\nA bar chart displays the count of each observations in a group. It is a useful visualization for nominal/categorical data.\n\n#this creates the grouped dataset\ngrouped_rural &lt;- rurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(Count = n()) \n\n#this plots the bar plot (note that the coord_flip swtiches the axis which is better for labeling in this case)\nggplot(grouped_rural, aes(x=ruca_code, y=Count)) + \n  geom_bar(stat = \"identity\") + coord_flip() + labs(\n    x = \"RUCA Code Category\",\n    y = \"Number of Census Tracts\",\n  )\n\n\n\n\n\n\n\n\n\n\n7.2.2 Map\n\n#here's our first map. Note that I'm using the raw data, not the grouped data.\ntm_shape(rurality) + tm_polygons(\"ruca_code\")\n\n\n\n\n\n\n\n\nQ4: What spatial patterns can you see in RUCA codes across NC? What does this map tell us that we can’t learn from the descriptive statistics table and non-spatial visualizations?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#creating-a-basic-table-for-descriptive-statistics-1",
    "href": "describing_data.html#creating-a-basic-table-for-descriptive-statistics-1",
    "title": "6  Describing Data",
    "section": "8.1 Creating a Basic Table for Descriptive Statistics",
    "text": "8.1 Creating a Basic Table for Descriptive Statistics\n\n## create a basic table of descriptive statistics for climate normals\nclimate_normals |&gt; st_drop_geometry() |&gt;\n  select(summer_temp) |&gt;\n  summarise(\n    n = n(),\n    num_na = sum(is.na(summer_temp)), \n    mean_temp = mean(summer_temp, na.rm = TRUE),\n    median_temp = median(summer_temp, na.rm = TRUE),\n    sd_temp = sd(summer_temp, na.rm = TRUE),\n    variance_temp = var(summer_temp, na.rm = TRUE),\n    mean_dev = mean(abs(summer_temp - mean(summer_temp, na.rm = TRUE)), na.rm = TRUE),\n    cv_temp = sd_temp / mean_temp,\n    min_temp = min(summer_temp, na.rm = TRUE),\n    max_temp = max(summer_temp, na.rm = TRUE),\n    skewness = skewness(summer_temp, na.rm = TRUE), \n    kurtosis = kurtosis(summer_temp, na.rm = TRUE)\n                               \n  ) |&gt; pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"US Summer Temperatures (1991-2020)\",\n  ) %&gt;%\n  fmt_number(\n    columns = everything(),\n    decimals = 2\n  )\n\n\n\n\n\n\n\nUS Summer Temperatures (1991-2020)\n\n\nStatistic\nValue\n\n\n\n\nn\n469,758.00\n\n\nnum_na\n0.00\n\n\nmean_temp\n71.86\n\n\nmedian_temp\n71.55\n\n\nsd_temp\n7.64\n\n\nvariance_temp\n58.36\n\n\nmean_dev\n6.26\n\n\ncv_temp\n0.11\n\n\nmin_temp\n29.91\n\n\nmax_temp\n98.04\n\n\nskewness\n−0.09\n\n\nkurtosis\n−0.25\n\n\n\n\n\n\n\nQ6: Based on the descriptive statistics in the table, describe the overall distribution of summer temperatures across the U.S., including central tendency, variability, and evidence of skewness or outliers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#visualizing-descriptive-statistics-1",
    "href": "describing_data.html#visualizing-descriptive-statistics-1",
    "title": "6  Describing Data",
    "section": "8.2 Visualizing Descriptive Statistics",
    "text": "8.2 Visualizing Descriptive Statistics\n\n8.2.1 Histogram\nA histogram is a graph that displays the frequency of observations within user-set “bins”. This can give us an understanding of the distribution of our data\n\n## create a histogram of summer temperatures\n\nggplot(climate_normals, aes(x = summer_temp)) +\n  geom_histogram(binwidth = 1, fill = \"lightgrey\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Frequency\"\n  ) \n\n\n\n\n\n\n\n\n\n\n8.2.2 Cumulative Frequency Plot\n\nggplot(climate_normals, aes(x = summer_temp)) + stat_ecdf() +\n  labs(\n    title = \"Cumulative Frequency Plot of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Cumulative Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\n8.2.3 Boxplot\nA boxplot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the data.\n\nggplot(climate_normals, aes(y = summer_temp)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  labs(\n    title = \"Boxplot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  )\n\n\n\n\n\n\n\n\n\n\n8.2.4 Violin Plot\nA violin plot is a method of plotting numeric data and can be understood as a combination of a boxplot and a kernel density plot. It shows the distribution of the data across different values.\n\nggplot(climate_normals, aes(x = 1, y = summer_temp)) +\n  geom_violin(fill = \"lightgrey\", bw = 1.2)  + geom_boxplot(width=0.1) +\n  labs(\n    title = \"Violin Plot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  ) + theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\n\n\n\n\nQ7: What unique information does each of the four visualizations (histogram, cumulative frequency plot, boxplot, and violin plot) reveal about the distribution of summer temperatures?\n\n\n8.2.5 Map\n\ntm_shape(climate_normals) + tm_dots(\"summer_temp\")\n\n\n\n\n\n\n\n\nQ8: What does this map show us about the pattern of summer temperature across the U.S",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html",
    "href": "describing_spatial_data.html",
    "title": "8  Describing Spatial Data",
    "section": "",
    "text": "8.1 Describing Tornadoes\nIn this chapter, we will build on the descriptive tools we learned in the Describing Data chapter. Spatial descriptive statistics summarize datasets by their spatial characteristics, not their attribute values. Combining traditional descriptive statistics with spatial descriptive statistics can expand our understanding of the dataset.\nWe will use the following spatial datasets:\nTo follow along with this tutorial, download the .Rmd template here. The template already includes a code chunk for loading libraries and reading in the data, along with labeled empty chunks for each section of the tutorial. As you work through the code examples below, add each set of commands to the chunk with the matching section heading in your template.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#orienting-ourselves-to-the-datasets",
    "href": "describing_spatial_data.html#orienting-ourselves-to-the-datasets",
    "title": "7  Describing Spatial Data",
    "section": "7.2 Orienting Ourselves to the Datasets",
    "text": "7.2 Orienting Ourselves to the Datasets\nThe ‘glimpse()’ function provides a quick overview of the structure and contents of a dataset. It displays the data types of each column and a preview of the data, making it easier to understand the dataset at a glance.\n\n## glimpse at the rurality data\nglimpse(rurality)\n\nRows: 2,672\nColumns: 4\n$ GEOID           &lt;chr&gt; \"37141920300\", \"37141990100\", \"37071031600\", \"37071031…\n$ ruca_code       &lt;chr&gt; \"Metropolitan high commuting\", \"Not coded\", \"Metropoli…\n$ nc_rural_center &lt;chr&gt; \"Rural\", \"Rural\", \"Not Rural\", \"Not Rural\", \"Rural\", \"…\n$ geometry        &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-78.15648 3..., MULTIPOLY…\n\n## glimpse at the climate normals data\nglimpse(climate_normals)\n\nRows: 469,758\nColumns: 2\n$ summer_temp &lt;dbl&gt; 56.25781, 56.25312, 56.27656, 56.40313, 56.46875, 57.36875…\n$ geometry    &lt;POINT [°]&gt; POINT (-124.6875 48.0625), POINT (-124.6875 48.10417…\n\n#glimpse at acs data\nglimpse(acs_tract_nc)\n\nRows: 2,672\nColumns: 54\n$ STATEFP          &lt;chr&gt; \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\", \"37\",…\n$ COUNTYFP         &lt;chr&gt; \"031\", \"031\", \"031\", \"031\", \"031\", \"031\", \"031\", \"119…\n$ TRACTCE          &lt;chr&gt; \"970402\", \"970502\", \"970805\", \"970903\", \"970101\", \"97…\n$ GEOID            &lt;chr&gt; \"37031970402\", \"37031970502\", \"37031970805\", \"3703197…\n$ NAME             &lt;chr&gt; \"9704.02\", \"9705.02\", \"9708.05\", \"9709.03\", \"9701.01\"…\n$ NAMELSAD         &lt;chr&gt; \"Census Tract 9704.02\", \"Census Tract 9705.02\", \"Cens…\n$ MTFCC            &lt;chr&gt; \"G5020\", \"G5020\", \"G5020\", \"G5020\", \"G5020\", \"G5020\",…\n$ FUNCSTAT         &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\"…\n$ ALAND            &lt;dbl&gt; 2395384, 6094915, 139248627, 5864308, 28018671, 97430…\n$ AWATER           &lt;dbl&gt; 1553995, 4244211, 421209, 30151466, 35773987, 1568180…\n$ INTPTLAT         &lt;chr&gt; \"+34.7239933\", \"+34.7517653\", \"+34.7817392\", \"+34.701…\n$ INTPTLON         &lt;chr&gt; \"-076.7087383\", \"-076.7387561\", \"-077.0184797\", \"-076…\n$ tract_name       &lt;chr&gt; \"Census Tract 9704.02, Carteret County, North Carolin…\n$ total_pop        &lt;dbl&gt; 1309, 2177, 1694, 1211, 965, 4244, 2306, 2720, 6076, …\n$ median_age       &lt;dbl&gt; 39.3, 56.1, 43.3, 62.1, 48.4, 55.9, 42.3, 28.3, 29.7,…\n$ pct_white        &lt;dbl&gt; 64.17, 93.11, 85.36, 85.47, 86.22, 72.64, 72.94, 6.32…\n$ pct_black        &lt;dbl&gt; 21.01, 2.30, 0.00, 0.66, 3.21, 17.86, 9.71, 45.96, 30…\n$ pct_aian         &lt;dbl&gt; 0.15, 0.00, 0.41, 0.41, 0.00, 2.00, 0.00, 0.00, 0.05,…\n$ pct_asian        &lt;dbl&gt; 0.00, 0.00, 1.89, 0.00, 0.00, 0.00, 4.68, 2.90, 1.50,…\n$ pct_nhpi         &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.17, 1.32, 0.00,…\n$ pct_other_race   &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 1.06, 0.22, 0.00, 0.00,…\n$ pct_two_races    &lt;dbl&gt; 12.15, 0.32, 10.68, 12.80, 0.83, 3.79, 8.24, 1.69, 0.…\n$ pct_hisp         &lt;dbl&gt; 2.52, 4.27, 1.65, 0.66, 9.74, 2.64, 4.03, 41.80, 63.8…\n$ pct_family_hh    &lt;dbl&gt; 39.31, 64.53, 73.30, 64.78, 65.03, 64.45, 62.60, 61.2…\n$ married_fam      &lt;dbl&gt; 19.08, 55.75, 55.45, 61.55, 45.08, 55.44, 42.31, 28.3…\n$ male_hh          &lt;dbl&gt; 3.71, 0.00, 3.81, 0.48, 9.84, 3.64, 2.88, 9.40, 8.66,…\n$ female_hh        &lt;dbl&gt; 16.52, 8.77, 14.03, 2.75, 10.11, 5.37, 17.40, 23.56, …\n$ pct_nonfam_hh    &lt;dbl&gt; 60.69, 35.47, 26.70, 35.22, 34.97, 35.55, 37.40, 38.7…\n$ nonfam_male_hh   &lt;dbl&gt; 37.00, 16.13, 15.26, 14.22, 18.31, 17.25, 13.37, 13.7…\n$ nonfam_fem_hh    &lt;dbl&gt; 23.69, 19.34, 11.44, 21.00, 16.67, 18.30, 24.04, 25.0…\n$ av_hh_size       &lt;dbl&gt; 1.64, 2.05, 2.31, 1.96, 2.37, 2.03, 2.21, 3.01, 2.89,…\n$ pct_less_hs      &lt;dbl&gt; 17.40, 7.95, 9.49, 0.00, 8.64, 18.64, 6.50, 19.05, 29…\n$ pct_hs           &lt;dbl&gt; 52.21, 53.92, 69.15, 41.87, 67.83, 59.63, 61.34, 60.9…\n$ pc_bach          &lt;dbl&gt; 30.38, 38.13, 21.36, 58.13, 23.54, 21.73, 32.17, 19.9…\n$ pct_unemp        &lt;dbl&gt; 5.67, 0.41, 0.86, 1.47, 1.45, 0.13, 0.95, 5.91, 8.52,…\n$ med_hh_inc       &lt;dbl&gt; 45391, 86583, 66548, 96198, 58750, 67195, 63393, 4844…\n$ gini_index       &lt;dbl&gt; 0.4094, 0.4039, 0.4377, 0.4689, 0.5036, 0.4248, 0.462…\n$ pct_owner        &lt;dbl&gt; 41.87, 87.64, 81.47, 86.11, 81.42, 70.29, 62.98, 25.1…\n$ pct_renter       &lt;dbl&gt; 58.13, 12.36, 18.53, 13.89, 18.58, 29.71, 37.02, 74.8…\n$ pct_vacant       &lt;dbl&gt; 19.57, 10.09, 18.90, 78.84, 45.29, 14.68, 8.29, 6.22,…\n$ med_year_built   &lt;dbl&gt; 1962, 1985, 1997, 1988, 1965, 1994, 1984, 1983, 1986,…\n$ pct_gas          &lt;dbl&gt; 18.95, 13.02, 7.36, 13.41, 31.15, 2.30, 3.56, 49.89, …\n$ pct_electric     &lt;dbl&gt; 77.21, 85.66, 85.01, 84.81, 53.83, 96.26, 95.67, 49.7…\n$ med_house_val    &lt;dbl&gt; 234100, 329900, 208800, 669300, 168500, 262600, 23640…\n$ med_rent         &lt;dbl&gt; 781, 1241, 1133, 1489, 1000, 762, 1184, 1355, 1309, 2…\n$ pct_pov          &lt;dbl&gt; 26.02, 4.10, 9.63, 4.21, 16.63, 9.50, 18.35, 16.09, 1…\n$ pct_drive_work   &lt;dbl&gt; 90.34, 89.37, 87.40, 75.43, 78.46, 85.78, 94.20, 86.3…\n$ pct_public_trans &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 4.12, 3.60,…\n$ bike_walk        &lt;dbl&gt; 3.22, 0.00, 2.95, 0.00, 2.25, 0.00, 1.40, 2.22, 0.00,…\n$ pct_wfh          &lt;dbl&gt; 6.44, 7.95, 8.17, 24.57, 8.36, 10.30, 4.12, 5.43, 11.…\n$ av_commute       &lt;dbl&gt; 17, 20, 26, 28, 32, 17, 23, 19, 25, 24, 25, 23, 25, 2…\n$ pct_no_veh       &lt;dbl&gt; 27.53, 0.57, 0.00, 3.07, 4.92, 2.97, 3.94, 7.52, 7.90…\n$ no_health_insur  &lt;dbl&gt; 19.39, 7.21, 15.13, 4.29, 7.85, 16.02, 6.16, 14.72, 4…\n$ geometry         &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-76.74294 3..., MULTIPOL…",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#creating-a-basic-table-for-descriptive-statistics",
    "href": "describing_spatial_data.html#creating-a-basic-table-for-descriptive-statistics",
    "title": "7  Describing Spatial Data",
    "section": "8.1 Creating a Basic Table for Descriptive Statistics",
    "text": "8.1 Creating a Basic Table for Descriptive Statistics\nThe ‘gt’ package in R is a tool to create “presentation-ready” tables.\n\n## create a basic table of descriptive statistics for RUCA codes\nrurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(n = n())  |&gt;  gt() \n\n\n\n\n\n\n\nruca_code\nn\n\n\n\n\nMetropolitan core\n1436\n\n\nMetropolitan high commuting\n406\n\n\nMetropolitan low commuting\n87\n\n\nMicropolitan core\n218\n\n\nMicropolitan high commuting\n134\n\n\nMicropolitan low commuting\n79\n\n\nNot coded\n12\n\n\nRural area\n225\n\n\nSmall town core\n39\n\n\nSmall town high commuting\n20\n\n\nSmall town low commuting\n16\n\n\n\n\n\n\n\nQ3. What is the mode of the ruca_code?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#visualizing-descriptive-statistics",
    "href": "describing_spatial_data.html#visualizing-descriptive-statistics",
    "title": "7  Describing Spatial Data",
    "section": "8.2 Visualizing Descriptive Statistics",
    "text": "8.2 Visualizing Descriptive Statistics\nOur descriptive statistics table has given us some useful information about our dataset. Visualizing our data can help us understand our descriptive statistics better.\n\n8.2.1 Bar Chart\nA bar chart displays the count of each observations in a group. It is a useful visualization for nominal/categorical data.\n\n#this creates the grouped dataset\ngrouped_rural &lt;- rurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(Count = n()) \n\n#this plots the bar plot (note that the coord_flip swtiches the axis which is better for labeling in this case)\nggplot(grouped_rural, aes(x=ruca_code, y=Count)) + \n  geom_bar(stat = \"identity\") + coord_flip() + labs(\n    x = \"RUCA Code Category\",\n    y = \"Number of Census Tracts\",\n  )\n\n\n\n\n\n\n\n\n\n\n8.2.2 Map\n\n#here's our first map. Note that I'm using the raw data, not the grouped data.\ntm_shape(rurality) + tm_polygons(\"ruca_code\")\n\n\n\n\n\n\n\n\nQ4: What does the map show us that we can’t see from the descriptive statistics table or the bar chart?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#creating-a-basic-table-for-descriptive-statistics-1",
    "href": "describing_spatial_data.html#creating-a-basic-table-for-descriptive-statistics-1",
    "title": "7  Describing Spatial Data",
    "section": "9.1 Creating a Basic Table for Descriptive Statistics",
    "text": "9.1 Creating a Basic Table for Descriptive Statistics\n\n## create a basic table of descriptive statistics for climate normals\nclimate_normals |&gt; st_drop_geometry() |&gt;\n  select(summer_temp) |&gt;\n  summarise(\n    n = n(),\n    num_na = sum(is.na(summer_temp)), \n    mean_temp = mean(summer_temp, na.rm = TRUE),\n    median_temp = median(summer_temp, na.rm = TRUE),\n    sd_temp = sd(summer_temp, na.rm = TRUE),\n    variance_temp = var(summer_temp, na.rm = TRUE),\n    mean_dev = mean(abs(summer_temp - mean(summer_temp, na.rm = TRUE)), na.rm = TRUE),\n    cv_temp = sd_temp / mean_temp,\n    min_temp = min(summer_temp, na.rm = TRUE),\n    max_temp = max(summer_temp, na.rm = TRUE),\n    skewness = skewness(summer_temp, na.rm = TRUE), \n    kurtosis = kurtosis(summer_temp, na.rm = TRUE)\n                               \n  ) |&gt; pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"US Summer Temperatures (1991-2020)\",\n  ) %&gt;%\n  fmt_number(\n    columns = everything(),\n    decimals = 2\n  )\n\n\n\n\n\n\n\nUS Summer Temperatures (1991-2020)\n\n\nStatistic\nValue\n\n\n\n\nn\n469,758.00\n\n\nnum_na\n0.00\n\n\nmean_temp\n71.86\n\n\nmedian_temp\n71.55\n\n\nsd_temp\n7.64\n\n\nvariance_temp\n58.36\n\n\nmean_dev\n6.26\n\n\ncv_temp\n0.11\n\n\nmin_temp\n29.91\n\n\nmax_temp\n98.04\n\n\nskewness\n−0.09\n\n\nkurtosis\n−0.25\n\n\n\n\n\n\n\nQ6: What does this descriptive statistics table tell us about the data?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#visualizing-descriptive-statistics-1",
    "href": "describing_spatial_data.html#visualizing-descriptive-statistics-1",
    "title": "7  Describing Spatial Data",
    "section": "9.2 Visualizing Descriptive Statistics",
    "text": "9.2 Visualizing Descriptive Statistics\n\n9.2.1 Histogram\nA histogram is a graph that displays the frequency of observations within user-set “bins”. This can give us an understanding of the distribution of our data\n\n## create a histogram of summer temperatures\n\nggplot(climate_normals, aes(x = summer_temp)) +\n  geom_histogram(binwidth = 1, fill = \"lightgrey\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Frequency\"\n  ) \n\n\n\n\n\n\n\n\n\n\n9.2.2 Cumulative Frequency Plot\n\nggplot(climate_normals, aes(x = summer_temp)) + stat_ecdf() +\n  labs(\n    title = \"Cumulative Frequency Plot of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Cumulative Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\n9.2.3 Boxplot\nA boxplot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the data.\n\nggplot(climate_normals, aes(y = summer_temp)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  labs(\n    title = \"Boxplot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  )\n\n\n\n\n\n\n\n\n\n\n9.2.4 Violin Plot\nA violin plot is a method of plotting numeric data and can be understood as a combination of a boxplot and a kernel density plot. It shows the distribution of the data across different values.\n\nggplot(climate_normals, aes(x = 1, y = summer_temp)) +\n  geom_violin(fill = \"lightgrey\", bw = 1.2)  + geom_boxplot(width=0.1) +\n  labs(\n    title = \"Violin Plot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  ) + theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\n\n\n\n\nQ7: How do visualizations help us learn more about a dataset compared to a descriptive statistics table\nQ8: How does each type of visualization give us additional information?\n\n\n9.2.5 Map\n\ntm_shape(climate_normals) + tm_dots(\"summer_temp\")\n\n\n\n\n\n\n\n\nQ9: What does this map show us that we don’t learn from the descriptive statistics table or the other visualizations?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "lab_1.html",
    "href": "lab_1.html",
    "title": "7  Lab 3",
    "section": "",
    "text": "7.1 Overview\nIn the Describing Data tutorial we worked with spatial datasets describing the rurality of North Carolina census tracts, summer temperatures across the United States, and a demographic variable measured at the census tract level in North Carolina. Spatial data is often aggregated to specific geographic units (for instance, to protect individual privacy or simplify a complex dataset). This aggregation influences the descriptive statistics we calculate.\nIn this lab, you will examine demographic and climate data at a different level of spatial aggregation and observe how the descriptive statistics change as a result (remember the underlying data values are not changing, only the geographic units used to summarize them). You will also explore an alternative way of defining “rural” and compare how this definition shapes your interpretation of the data.\nRemember that you can use the Command Compendium to help you modify R commands",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab 3</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "8  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "lab_1.html#specifications",
    "href": "lab_1.html#specifications",
    "title": "7  Lab 3",
    "section": "7.2 Specifications",
    "text": "7.2 Specifications\nThis lab is designed to assess the Concept 3 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nHTML and RMD versions the R Markdown file have been submitted.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output) so that the analysis is easy to read and understand.\n\n\nDescribing Rurality: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation\n\n\nDescribing Climate: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation\n\n\nDescribing Demographics: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab 3</span>"
    ]
  },
  {
    "objectID": "lab_1.html#lab-instructions",
    "href": "lab_1.html#lab-instructions",
    "title": "7  Lab 3",
    "section": "7.3 Lab Instructions",
    "text": "7.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab3.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n## load necessary libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sf)\nlibrary(e1071)\nlibrary(tmap)\n\n\n#read in data\nrurality &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1aVDPgiZLUnKGb5k2SQo7ScDO2WNvX3Nd\")\ncounty_climate_normals &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1sf0cmMaeHGra7AQheYAjTiC3wGCzB4FG\")\nacs_county_nc &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1Cv9iUFFi2RPvDBiuYGolY7dDuUR7_raN\")\n\nAdd “Describing Rurality” as a third-level header. In this section you will compute descriptive statistics and visualizations for North Carolina census tracts using the North Carolina Rural Center’s definition of rurality (rural = fewer than 250 people per square mile). The NC Rural Center variable is named nc_rural_center in the dataset. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of rurality by census tract\nCreate a bar chart of rurality by census tract\nCreate a map of rurality by census tract\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the bar chart and descriptive statistics tell us about the data\nDescribe what the map tells us about the spatial pattern of the data\nCompare the NC Rural Center definition of rural with the USDA RUCA codes, describing how the descriptive statistics differ and how the spatial patterns differ across the state\n\nAdd “Describing Climate” as a third-level header. In this section you will compute descriptive statistics and visualizations for average summer temperatures per county in the U.S. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of average summer temperature by U.S county\nCreate two appropriate non-map data visualizations of average summer temperature by U.S. county\nCreate a map of average of summer temperature by U.S. county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about average summer temperature (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to latitude/longitude (point-level) data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data\n\n\nAdd “Describing Demographics” as a third-level header. In this section you will compute descriptive statistics and visualizations for your selected variable in Mini Challenge #1. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of your variable by NC county\nCreate two appropriate non-map data visualizations of your variable by NC county\nCreate a map of your variable by NC county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about your variable (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to the census tract data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Lab 3</span>"
    ]
  },
  {
    "objectID": "command.html",
    "href": "command.html",
    "title": "5  Command Compendium",
    "section": "",
    "text": "5.1 Basic Data Manipulation\nThis chapter is designed to help you easily craft commands.\nggplot color options",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "14  Labs",
    "section": "",
    "text": "14.1 Lab 2",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "labs.html#specifications",
    "href": "labs.html#specifications",
    "title": "8  Lab 3",
    "section": "8.2 Specifications",
    "text": "8.2 Specifications\nThis lab is designed to assess the Concept 3 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nHTML and RMD versions the R Markdown file have been submitted.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output) so that the analysis is easy to read and understand.\n\n\nDescribing Rurality: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation\n\n\nDescribing Climate: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation\n\n\nDescribing Demographics: Analysis demonstrates competency through correct descriptive statistics, appropriate visualizations, a map, and an accurate written interpretation",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab 3</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-instructions",
    "href": "labs.html#lab-instructions",
    "title": "8  Lab 3",
    "section": "8.3 Lab Instructions",
    "text": "8.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab3.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n## load necessary libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sf)\nlibrary(e1071)\nlibrary(tmap)\n\n\n#read in data\nrurality &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1aVDPgiZLUnKGb5k2SQo7ScDO2WNvX3Nd\")\ncounty_climate_normals &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1sf0cmMaeHGra7AQheYAjTiC3wGCzB4FG\")\nacs_county_nc &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1Cv9iUFFi2RPvDBiuYGolY7dDuUR7_raN\")\n\nAdd “Describing Rurality” as a third-level header. In this section you will compute descriptive statistics and visualizations for North Carolina census tracts using the North Carolina Rural Center’s definition of rurality (rural = fewer than 250 people per square mile). The NC Rural Center variable is named nc_rural_center in the dataset. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of rurality by census tract\nCreate a bar chart of rurality by census tract\nCreate a map of rurality by census tract\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the bar chart and descriptive statistics tell us about the data\nDescribe what the map tells us about the spatial pattern of the data\nCompare the NC Rural Center definition of rural with the USDA RUCA codes, describing how the descriptive statistics differ and how the spatial patterns differ across the state\n\nAdd “Describing Climate” as a third-level header. In this section you will compute descriptive statistics and visualizations for average summer temperatures per county in the U.S. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of average summer temperature by U.S county\nCreate two appropriate non-map data visualizations of average summer temperature by U.S. county\nCreate a map of average of summer temperature by U.S. county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about average summer temperature (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to latitude/longitude (point-level) data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data\n\n\nAdd “Describing Demographics” as a third-level header. In this section you will compute descriptive statistics and visualizations for your selected variable in Mini Challenge #1. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of your variable by NC county\nCreate two appropriate non-map data visualizations of your variable by NC county\nCreate a map of your variable by NC county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about your variable (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to the census tract data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lab 3</span>"
    ]
  },
  {
    "objectID": "command.html#descriptive-statistics",
    "href": "command.html#descriptive-statistics",
    "title": "5  Command Compendium",
    "section": "5.2 Descriptive Statistics",
    "text": "5.2 Descriptive Statistics\n\n5.2.1 Descriptive Statistics Tables\n\n#requires e1071 package\n#requires gt package\n## descriptive statistics for quantitative data\n  DATASET_OBJECT |&gt; #whatever the object is named\n  st_drop_geometry() |&gt; #gets rid of geometry column\n  select(VARIABLE_OF_INTEREST) |&gt; #field name of variable of interest\n  summarise(                  #summarizes variable of interest. You can add/remove\n    n = n(),\n    num_na = sum(is.na(VARIABLE_OF_INTEREST)), \n    mean = mean(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    median = median(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    sd = sd(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    variance = var(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    mean_dev = mean(abs(VARIABLE_OF_INTEREST - mean(VARIABLE_OF_INTEREST, na.rm = TRUE)), na.rm = TRUE),\n    cv = sd / mean,\n    min = min(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    max = max(VARIABLE_OF_INTEREST, na.rm = TRUE),\n    skewness = skewness(VARIABLE_OF_INTEREST, na.rm = TRUE), \n    kurtosis = kurtosis(VARIABLE_OF_INTEREST, na.rm = TRUE)\n                               \n  ) |&gt; \n    #this starts the table formatting.\n  pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"TITLE\", ##Add your title here\n  ) %&gt;%\n  fmt_number(\n    columns = everything(),\n    decimals = 2 ##You can change number of decimals\n  )\n\n## Descriptive statistics table for categorical data\nDATASET_OBJECT |&gt; #whatever the object is named\n  st_drop_geometry() |&gt; #gets rid of geometry column\n  group_by(ruca_code) |&gt; #variable that you want to summarize\n  summarise(n = n())  |&gt; #calculates the count\n  gt() #formats into a table\n\n\n\n5.2.2 Histogram\n\n#requires ggplot2 package\n## Create a basic histogram. \n## You will likely need to modify the binwidth command\nggplot(DATASET_OBJECT, aes(x = VARIABLE OF INTEREST)) +\n  geom_histogram(binwidth = 1, fill = \"lightgrey\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"TITLE\",\n    x = \"X-AXIS LABEL\",\n    y = \"Y-AXIS LABEL\"\n  ) \n\n\n\n5.2.3 Boxplot\n\n#requires ggplot2 package\n# create basic boxplot\nggplot(DATASET_OBJECT, aes(y = VARIABLE OF INTEREST)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  labs(\n    title = \"TITLE\",\n    y = \"Y-AXIS LABEL\"\n  )\n\n\n\n5.2.4 Violin Plot\n\n#requires ggplot2 package\n##you will likely need to modify the bw\nggplot(DATASET_OBJECT, aes(x = 1, y = VARIABLE_OF_INTEREST)) +\n  geom_violin(fill = \"lightgrey\", bw = 1.2)  + geom_boxplot(width=0.1) +\n  labs(\n    title = \"TITLE\",\n    y = \"Y AXIS LABEL\"\n  ) + theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n5.2.5 Bar Chart\n\n#requires ggplot2 package\n##YOU NEED TO FIRST GROUP THE DATA\ngrouped_data &lt;- DATASET_OBJECT |&gt; st_drop_geometry() |&gt; \n  group_by(VARIABLE_OF_INTEREST) |&gt;\n  summarise(Count = n()) \n\n#this plots the bar plot (note that the coord_flip swtiches the axis which is optional\nggplot(grouped_data, aes(x=VARIABLE_OF_INTEREST, y=Count)) + \n  geom_bar(stat = \"identity\") + coord_flip()  + labs(\n    x = \"X_AXIS_LABEL\",\n    y = \"Y_AXIS_LABEL\",\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "command.html#maps",
    "href": "command.html#maps",
    "title": "5  Command Compendium",
    "section": "5.3 Maps",
    "text": "5.3 Maps\n\n#required tmap package\n## classification styles\n## \"equal\"\n## \"fisher\"\n## \"pretty\"\n## \"quantile\"\n\n#command to see color palettes\ncols4all::c4a_gui()\n\n##make a basic point map\ntm_shape(DATABASE_OBJECT) + tm_dots(fill = \"VARIABLE_OF_INTEREST\",\n                fill.scale = tm_scale_intervals(values = \"brewer.COLORPALETTE\", style = \"CLASSIFICATION\", n= NUMBEROFCLASSES)) \n\n##make a basic polygon map\ntm_shape(DATABASE_OBJECT) + tm_polygon(fill = \"VARIABLE_OF_INTEREST\",\n                fill.scale = tm_scale_intervals(values = \"brewer.COLOR_PALETTE\", style = \"CLASSIFICATION\", n= NUMBEROFCLASSES)) \n\n\n## make an interactive map\ntmap_mode(mode = \"view\")\n\n##go back to static map \ntmap_mode(mode = \"plot\")\n\n\n## add title \nMAPCOMMANDS + tm_title(\"TITLE\")\n\n\n## Add transparency\ntm_shape(DATABASE_OBJECT) + tm_polygon(fill = \"VARIABLE_OF_INTEREST\",fill_alpha = VALUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "command.html#basic-data-manipulation",
    "href": "command.html#basic-data-manipulation",
    "title": "5  Command Compendium",
    "section": "",
    "text": "5.1.1 Creating Objects\nCreating an object allows you to store the output of a command in the environment. This means you can reuse the result later by referencing the object name, instead of rerunning the command every time.\n\n## When this command runs, it will show up in the environment\nchart &lt;- ggplot(DATASET_OBJECT, aes(y = VARIABLE OF INTEREST)) +\n  geom_boxplot(fill = \"lightgrey\")\n\n##now I can \"call\" the object\nchart\n\n\n\n5.1.2 Filtering Data\nTo create our analytical dataset we often need to subset a larger dataset. Filtering allows us to simplify. We always want to save filtered data as objects so that we can reference them in other commands.\n\n#to select certain columns\ncolumns &lt;- DATASET |&gt; select(COLUMN1, COLUMN2, COLUMN3)\n\n### IF VALUES ARE STRING (NON-NUMERIC) THEY MUST BE IN \"\" \n\n#to filter by values in a certain column\nvalues &lt;- DATASET |&gt; filter(COLUMN == VALUE)\n\n# to filter by multiple conditions. \n#and\nvalues &lt;- DATASET |&gt; filter(COLUMN == VALUE & COLUMN2 == VALUE2)\n\n#or \nvalues &lt;- DATASET |&gt; filter(COLUMN == VALUE | COLUMN2 == VALUE2)\n\n#in \nvalues &lt;- DATASET |&gt; filter(COLUMN %in% c(VALUE1, VALUE2, VALUE3))\n\n#numeric operator\nvalues &lt;- DATASET |&gt; filter(COLUMN &lt; 2000)\n\n\n\n5.1.3 Making New Columns\n\n#creating new columns\ndataset &lt;- DATASET |&gt; mutate(NEWCOLUMN = COMMAND, NEWCOLUMN2 = COMMAND)\n\n\n\n5.1.4 Renaming Columns\n\n#renaming columns\nclean_data &lt;- DATASET |&gt; rename(\n  new_name1 = old_name1,\n  new_name2 = old_name2\n)\n\n\n\n5.1.5 Grouping and Summarizing Data\n\nsummary_data &lt;- DATASET |&gt; \n  group_by(CATEGORY_COLUMN) |&gt; \n  summarize(\n    mean_value = mean(NUMERIC_COLUMN, na.rm = TRUE),\n    count = n()\n  )\n\n\n\n5.1.6 Creating a Spatial Object\n\n#assumes a nonspatial object with a longitude and latitude field\ngrouped_pm_sf &lt;- YOUR_NEWOBJECT |&gt; st_as_sf(coords= c(\"lat\", \"lon\"), crs = 4326)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-3",
    "href": "labs.html#lab-3",
    "title": "14  Labs",
    "section": "14.2 Lab 3",
    "text": "14.2 Lab 3\n\n14.2.1 Overview\nIn the Describing Spatial Data tutorial we learned how to expand our descriptive statistics toolkit to include spatial descriptive statistics.\nIn this lab, you will practice calculating spatial descriptive statistics on a dataset representing the location of wind turbines in the continental US (variable names are here) and Covid-19 cases and deaths by US county centroid on April 1, 2020 and July 1, 2020.\nRemember that you can use the Command Compendium to help you modify R commands\n\n\n14.2.2 Specifications\nThis lab is designed to assess the Concept 3 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nStudent submits HTML and RMD versions the R Markdown file.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output). Minor formatting issues don’t prevent the work from being read and understood.\n\n\nDescribing Covid: Student produces descriptive statistics tables, calculates weighted means, and creates maps with mean centers for cases and deaths, including manual legend entries. Written interpretation demonstrates understanding of the distribution, spatial patterns, and why an unweighted mean center isn’t meaningful.\n\n\nDescribing Wind Turbines: Student produces a descriptive statistics table and one non-map visualization, calculates spatial descriptive statistics (mean center, standard deviational ellipse, weighted mean center), and creates a map with manual legend entries. Written interpretation demonstrates understanding of the variable’s distribution and spatial patterns.\n\n\n\n\n\n14.2.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab3.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n## load necessary libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sf)\nlibrary(e1071)\nlibrary(tmap)\nlibrary(maptiles)\nlibrary(spdep)\n\n\n#read in data\n\n##cumulative covid cases and deaths on April 1, 2020\ncovid_deaths_04012020 &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1spBLqVxa25a7FDo7U7aNDJF2t2N0-uXv\")\n\n##cumulative covid cases and deaths on July 1, 2020\ncovid_deaths_07012020 &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1ZNka0ffMXSVAqVDCj1R-oYh37BH9ziAZ\")\n\n##wind turbines across the continental US\nus_wind_turbine_locs &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1Cd9Nl1cNkGD_F9L68EqefKtFx7X7-IpE\")\n\nAdd “Describing Covid” as a third-level header. In each of the covid datasets, there is a variable named cases which represents the cumulative cases in each county on the given date and a variable named deaths which represents the cumulative deaths in each county on the given date. Note that the dataset only includes counties that had at least one case or death. In this section you will compute descriptive statistics for cases and deaths on each date, compute spatial descriptive statistics, and create a well designed map. Your code chunk should:\n\nCreate a descriptive statistics table of cases and deaths for each dataset\nCompute a weighted mean for cases and deaths on each date\nCreate a map for the April 2020 data that symbolizes cases by county, as well as the mean center for cases AND deaths. Make sure that you add manual legend entries for the mean center of cases and deaths.\nCreate a map for the July 2020 data that symbolizes cases by county, as well as the mean center for cases AND deaths. Make sure that you add manual legend entries for the mean center of cases and deaths.\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nSummarize what the descriptive statistics reveal about the distribution of Covid cases and deaths\nExplain why creating a meaningful histogram or boxplot for this data would be challenging\nDescribe what the maps (including the spatial descriptive statistics) reveal about the spatial distribution of Covid cases and deaths.\nExplain why an unweighted mean center isn’t meaningful in this context (consider the unit of observation)\n\nAdd a “Describing Wind Turbines” header. In this section, you will compute descriptive and spatial descriptive statistics for the location and characteristics of wind turbines across the continental US. Your code chunk should:\n\nCreate a descriptive statistics table for one quantitative variable in the turbine dataset.\nCreate one non-map data visualization for your variable of interest\nCalculate mean center of wind turbines, standard deviational ellipse of wind turbines, and weighted mean center based on your variable of interest.\nCreate a map that symbolizes your variable of interest, the mean center of wind turbines, the standard deviational ellipse of wind turbines, and the weighted mean center. Make sure that you add manual legend entries for the mean center and weighted mean center (you do not need to add a legend entry for the standard deviational ellipse).\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nSummarize what the descriptive statistics and non-map visualization reveal about the distribution of your variable\nDescribe what the maps (including the spatial descriptive statistics) reveal about the spatial distribution of wind turbines and your variable of interest\n\nKnit your .Rmd and make sure the formatting and organization are clear in the knitted .html document",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#section",
    "href": "describing_spatial_data.html#section",
    "title": "7  Describing Spatial Data",
    "section": "8.1 ",
    "text": "8.1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#describing-tornadoes",
    "href": "describing_spatial_data.html#describing-tornadoes",
    "title": "8  Describing Spatial Data",
    "section": "",
    "text": "8.1.1 Traditional Descriptive Statistics\nOpen the tornado object by double-clicking it in the environment. Important variables in the dataset are:\n\n\n\nField Name\nDescription\n\n\n\n\nyr\nYear of occurrence\n\n\nst\nState\n\n\nmag\nRating on Enhanced Fujita Scale\n\n\ninj\nNumber of injuries\n\n\nfat\nNumber of fatalities\n\n\nlen\nLength in miles\n\n\nwid\nWidth in yards\n\n\n\nQ1. What does each observation (each row) represent? How do you know?\nQ2. Create a descriptive statistics table for a categorical variable in the dataset. Create one data visualization to compliment the statistics table.\nQ3. Create a descriptive statistics table for a quantitative variable in the dataset. Create one data visualization to compliment the statistics table.\n\n\n8.1.2 Spatial Descriptive Statistics\n\n8.1.2.1 Mean Center\nThe mean center of a spatial dataset represents the average location of a set of points and is calculated by taking the average of all the x-coordinates and all the y-coordinates in the dataset.\n\n#calculate mean center of all tornados in dataset\ntorn_mean_center &lt;- center_mean(torn)\n\n#map tornadoes and mean center\ntm_shape(torn) + tm_dots() + tm_shape(torn_mean_center) + tm_dots(fill = \"blue\") + tm_add_legend(\n    type = \"symbols\",\n    labels = \"Mean Center\",\n    fill = \"blue\"\n  )\n\n\n\n\n\n\n\n\nQ4. What does the location of our mean center tell us about the spatial distribution of tornadoes in the United States?\n\n\n8.1.2.2 Weighted Mean Center\nThe weighted mean center calculates the average location, but allows you to select a variable to weight by. The calculation then gives more influence to features with larger values of that variable. For instance, we could give higher weight to fatal tornadoes\n\n## calculate weighted mean center using \"fat\" variable\ntorn_w_mean_center &lt;- center_mean(torn, weight = torn$fat)\n\n\n##map mean center and weighted mean center\ntm_shape(torn) + tm_dots() + tm_shape(torn_mean_center) + tm_dots(fill = \"blue\", ) + tm_shape(torn_w_mean_center) + tm_dots(fill = \"red\") + tm_add_legend(\n    type = \"symbols\",\n    labels = c(\"Mean Center\", \"Weighted Mean Center\"),\n    fill = c(\"blue\", \"red\")\n  )\n\n\n\n\n\n\n\n\nQ5. What does the difference between the mean center and the weighted mean center (weighted by fatalities) tell us about the spatial distribution of fatal tornadoes?\nQ6. Using the code below, calculate and map the difference in the weighted mean center for tornadoes before 2000 and after 2000. What does this tell us about how the spatial pattern of tornadoes has changed over time?\n\n#tornadoes before 2000\ntorn_b_2000 &lt;- torn |&gt; filter(yr &lt; 2000)\n\n#tornados after 2000\ntorn_a_2000 &lt;- torn |&gt; filter(yr &gt;= 2000)\n\n\n\n8.1.2.3 Standard Deviational Ellipse\nThe standard deviational ellipse is a method for summarizing the spatial central tendency, dispersion, and directional trends. The ellipse visually represents the spread of the data and the direction of the spread. It is centered on the mean center and its axes represent the standard deviation of the x and y coordinates.\n\n#calculate standard ellipse values\nstd_ellip_torn &lt;- std_dev_ellipse(torn)\n\n#create an ellipse of those values\nstd_ellip_torn &lt;- sfdep::st_ellipse(geometry =std_ellip_torn,\n                                   sx = std_ellip_torn$sx,\n                                   sy = std_ellip_torn$sy,\n                                   rotation = -std_ellip_torn$theta)\n#map the ellipse with transparency\ntm_shape(torn) + tm_dots() + tm_shape(std_ellip_torn) + tm_polygons(fill_alpha = .5)\n\n\n\n\n\n\n\n\nQ6. What does the standard deviational ellipse tell us about the direction and spread of the tornado dataset?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "describing_spatial_data.html#mini-challenge",
    "href": "describing_spatial_data.html#mini-challenge",
    "title": "8  Describing Spatial Data",
    "section": "8.2 Mini-Challenge",
    "text": "8.2 Mini-Challenge\nThis challenge will ask you to calculate a weighted mean center for different for a dataset on North Carolina hospitals. We will focus on a few variables:\n\n\n\nField Name\nDescription\n\n\n\n\nhgenlic\nTotal hospital general beds\n\n\nrehabhlic\nTotal hospital rehab beds\n\n\npsylic\nTotal hospital psych beds\n\n\nnfgenlic\nTotal nursing facility general beds\n\n\n\n\nOpen the nc_hosp dataset. What does each row represent?\nCalculate the mean center of all hospitals and the weighted mean center based on the four variables of interest\nCreate a map that displays the variability in the mean centers. Make sure to include a legend",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing Spatial Data</span>"
    ]
  },
  {
    "objectID": "command.html#descriptive-spatial-statistics",
    "href": "command.html#descriptive-spatial-statistics",
    "title": "5  Command Compendium",
    "section": "5.4 Descriptive Spatial Statistics",
    "text": "5.4 Descriptive Spatial Statistics\n\n5.4.1 Mean Center and Weighted Mean Center\n\n#requires sfdep package\n\n#calculate mean center\nDATASET_mean_center &lt;- center_mean(DATASET)\n\n#calculate weighted mean center\nDATASET_mean_center &lt;- center_mean(DATASET, weight = DATASET$VARIABLE)\n\n\n\n5.4.2 Standard Deviational Ellipse\n\n#requires sfdep package\n#calculate standard ellipse values\nstd_ellip_DATASET &lt;- std_dev_ellipse(DATASET)\n\n#create an ellipse of those values\nstd_ellip_DATASET &lt;- sfdep::st_ellipse(geometry =std_ellip_DATASET,\n                                   sx = std_ellip_DATASET$sx,\n                                   sy = std_ellip_DATASET$sy,\n                                   rotation = -std_ellip_DATASET$theta)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-4",
    "href": "labs.html#lab-4",
    "title": "14  Labs",
    "section": "14.3 Lab 4",
    "text": "14.3 Lab 4\n\n14.3.1 Overview\nIn the Probability tutorial we learned about calculating empirical probability using historical data and applying the binomial and normal probability distributions.\nIn this lab, you will practice applying the binomial probability distribution using daily water height data from the USGS water gauge in Bolin Creek from 2015-2025. You will also make a probability map of the probability of low January temperatures across North Carolina by applying the normal distribution.\nRemember that you can use the Command Compendium to help you modify R commands.\n\n\n14.3.2 Specifications\nThis lab is designed to assess the Concept 4 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nHTML and RMD versions the R Markdown file have been submitted.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output) so that the analysis is easy to read and understand.\n\n\nFlood Risk at Bolin Creek: Student creates a descriptive statistics table for daily water height, calculates empirical probability of a flood risk day, estimates the most likely number of flood risk days over 25 days, and computes cumulative probability of more than 3 flood risk days. Written interpretation demonstrates understanding of the results.\n\n\nLow Temperature Probabilities Across NC: Student calculates the probability of January days with maximum temperature below 30°F at each ASOS station and produces a probability map for the state. Written interpretation demonstrates understanding of the probabilities and spatial patterns.\n\n\n\n\nCreate a new .Rmd named “LASTNAME_lab4.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n#load libraries\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(tigris)\n\n#bolin creek daily data from 2015-2025\nbolin_creek_data &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=1eEzriJ5XzR-aS74nJmyiQFVrmV4T3cel\")\n\n#NC asos station data from 2000-2020\nasos_jan_data &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=1_zE1kcC7YY083R6rv20sqeN1l2dCdmhi\")\n\n\n\n\n14.3.3 Lab Instructions\n\nAdd a third-level header called “Flood Risk at Bolin Creek” and a new code chunk below the header. In this code chunk you should:\n\nCreate a descriptive statistics table for daily water height at the Bolin Creek USGS station\nCalculate the empirical probability of a “flood risk day” at Bolin Creek. A flood risk day is defined as a day where the water height is more than 1ft higher than the median of daily height for the period of records (2015-2025).\nUse this empirical probability to determine the most likely number of days of flood risk over a 25 day period.\nCalculate the cumulative probability of having more than 3 flood risk days over a period of 25 days.\n\nBelow this code chunk, write 1-2 paragraphs that address the following\n\nExplain in plain language what the empirical probability of a flood risk day represents\n\nWhat does this probability tell you about how often flood risk occurs at Bolin Creek?\n\nDescribe the output of the binomial calculation and explains which number of flood risk days is most likely in a 25-day period and why this makes sense given the empirical probability\nInterpret the cumulative probability of having more than 3 flood risk days in 25 days and explain what this means in terms of real-world flood risk.\n\nAdd a third-level header called “Low Temperature Probabilities in NC” and add a new code chunk below the header. In this code chunk you should:\n\nUse the normal distribution to calculate the probability (for each ASOS station) of having a January day with a maximum temperature below 40 degrees F.\nCreate a probability map for the full state\n\nBelow this code chunk, write 1-2 paragraphs that address the following:\n\nAre there areas of the state with particularly high or low probabilities? What might explain these differences?\nHow could these patterns inform planning or preparedness for cold weather events?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "tmap.html",
    "href": "tmap.html",
    "title": "7  Tmap Interlude",
    "section": "",
    "text": "7.1 Visual Variables\nMapping is an extremely important tool for describing and visualizing spatial data. Maps are particularly useful because they are able to summarize variation in both values and location (space). You (as a newly deputized cartographer) have a lot of control over how a map functions by making decisions on things like classification schemes, colors, layout, and design.\nThis tutorial is designed to introduce you the main functionalities of the tmap package. We will use census block group data to Orange, Durham, and Wake counties. This file will tell you what each variable name means.\nTo follow along with this tutorial, download the .Rmd template here. The template already includes a code chunk for loading libraries and reading in the data, along with labeled empty chunks for each section of the tutorial. As you work through the code examples below, add each set of commands to the chunk with the matching section heading in your template.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#visual-variables",
    "href": "tmap.html#visual-variables",
    "title": "7  Tmap Interlude",
    "section": "",
    "text": "7.1.1 Color\nWe are already familiar with making a basic tmap using color to visualize patterns:\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\")\n\n\n\n\n\n\n\n\n\n\n7.1.2 Size\nWe can also represent variables by size\n\ntm_shape(acs_tract_nc) + tm_symbols(size = \"total_pop\")\n\n\n\n\n\n\n\n\n\n\n7.1.3 Size and Color\nOr both!\n\ntm_shape(acs_tract_nc) + tm_symbols(size = \"total_pop\", fill = \"total_pop\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#color-palettes",
    "href": "tmap.html#color-palettes",
    "title": "7  Tmap Interlude",
    "section": "7.2 Color Palettes",
    "text": "7.2 Color Palettes\nWe can also modify the color palette. To see all the available color palettes, you should run the command below\n\ncols4all::c4a_gui()\n\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\", fill.scale = tm_scale_intervals(values = \"bu_pu\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#classification-schemes",
    "href": "tmap.html#classification-schemes",
    "title": "7  Tmap Interlude",
    "section": "7.3 Classification Schemes",
    "text": "7.3 Classification Schemes\nOne of the biggest decisions we can make is how to classify our data. You can see in the maps above that tmap defaults to an equal interval color scheme. However, this is often not the best way to classify the data. Your decision about classification should be made based on the distribution of the data. Let’s look at the distribution of the median age variable\n\nggplot(acs_tract_nc, aes(x = median_age)) + geom_histogram()\n\n\n\n\n\n\n\n\nBecause so much of our variation is between 30-45, using an equal interval color palette makes it difficult to visualize the majority of the variation. See how different the map looks when we use a quantile classification scheme?\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\", fill.scale = tm_scale_intervals(values = \"bu_pu\", style = \"quantile\"))\n\n\n\n\n\n\n\n\nThe following classification schemes will cover most of your uses:\n\nequal\npretty\nquantile\nfisher (natural breaks)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#interactive-mapping",
    "href": "tmap.html#interactive-mapping",
    "title": "7  Tmap Interlude",
    "section": "7.4 Interactive Mapping",
    "text": "7.4 Interactive Mapping\nSo far, we have only made static maps. However, tmap has the availability to make interactive maps as well.\n\n## make an interactive map\ntmap_mode(mode = \"view\")\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\", fill.scale = tm_scale_intervals(values = \"bu_pu\", style = \"quantile\"))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#formatting",
    "href": "tmap.html#formatting",
    "title": "7  Tmap Interlude",
    "section": "7.5 Formatting",
    "text": "7.5 Formatting\n\n7.5.1 Map 1: Adding Transparency and additional classes\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\", fill_alpha = .3, fill.scale = tm_scale_intervals(values = \"bu_pu\", style = \"quantile\", n = 7))\n\n\n\n\n\n\n\n\n7.5.2 Map 2: Adding a Basemap and a Title\n\n#change back to static plot\ntmap_mode(mode = \"plot\")\n\ntm_shape(acs_tract_nc) + tm_polygons(fill = \"median_age\",fill.scale = tm_scale_intervals(values = \"bu_pu\", style = \"quantile\", n = 5), fill_alpha = .3) +\ntm_title(\"Median Age by Census Block Group\") + tm_basemap(\"OpenStreetMap\")\n\n\n\n\n\n\n\n\nThe following basemaps will cover most of your uses:\n\n“CartoDB.VoyagerOnlyLabels” - Labels Only\n“CartoDB.PositronNoLabels”- No Labels\n“OpenStreetMap” - Labeled Basemap\n“Esri.WorldImagery” - Satellite",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "tmap.html#mini-challenge",
    "href": "tmap.html#mini-challenge",
    "title": "7  Tmap Interlude",
    "section": "7.6 Mini Challenge",
    "text": "7.6 Mini Challenge\nUsing a different variable in the ACS data, create a well-designed map where you select a classification scheme, and add transparency, a title, and a basemap.\nCreate an interactive and static version of the map.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tmap Interlude</span>"
    ]
  },
  {
    "objectID": "describing_data.html#describing-rurality",
    "href": "describing_data.html#describing-rurality",
    "title": "6  Describing Data",
    "section": "6.2 Describing Rurality",
    "text": "6.2 Describing Rurality\nOpen the rurality table by double clicking in the environment.\nQ1. What does each observation (each row) represent? How do you know?\nQ2. Look at the ruca_code variable. What level of measurement is this variable? How does that restrict what descriptive statistics we can calculate about the data?\n\n6.2.1 Creating a Basic Table for Descriptive Statistics\nThe ‘gt’ package in R is a tool to create “presentation-ready” tables.\n\n## create a basic table of descriptive statistics for RUCA codes\nrurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(n = n())  |&gt;  gt() \n\n\n\n\n\n\n\nruca_code\nn\n\n\n\n\nMetropolitan core\n1436\n\n\nMetropolitan high commuting\n406\n\n\nMetropolitan low commuting\n87\n\n\nMicropolitan core\n218\n\n\nMicropolitan high commuting\n134\n\n\nMicropolitan low commuting\n79\n\n\nNot coded\n12\n\n\nRural area\n225\n\n\nSmall town core\n39\n\n\nSmall town high commuting\n20\n\n\nSmall town low commuting\n16\n\n\n\n\n\n\n\nQ3. What is the mode of the ruca_code?\n\n\n6.2.2 Visualizing Descriptive Statistics\nOur descriptive statistics table has given us some useful information about our dataset. Visualizing our data can help us understand our descriptive statistics better.\n\n6.2.2.1 Bar Chart\nA bar chart displays the count of each observations in a group. It is a useful visualization for nominal/categorical data.\n\n#this creates the grouped dataset\ngrouped_rural &lt;- rurality |&gt; st_drop_geometry() |&gt; \n  group_by(ruca_code) |&gt;\n  summarise(Count = n()) \n\n#this plots the bar plot (note that the coord_flip swtiches the axis which is better for labeling in this case)\nggplot(grouped_rural, aes(x=ruca_code, y=Count)) + \n  geom_bar(stat = \"identity\") + coord_flip() + labs(\n    x = \"RUCA Code Category\",\n    y = \"Number of Census Tracts\",\n  )\n\n\n\n\n\n\n\n\n\n\n6.2.2.2 Map\n\n#here's our first map. Note that I'm using the raw data, not the grouped data.\ntm_shape(rurality) + tm_polygons(\"ruca_code\")\n\n\n\n\n\n\n\n\nQ4: What spatial patterns can you see in RUCA codes across NC? What does this map tell us that we can’t learn from the descriptive statistics table and non-spatial visualizations?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#describing-climate-normals",
    "href": "describing_data.html#describing-climate-normals",
    "title": "6  Describing Data",
    "section": "6.3 Describing Climate Normals",
    "text": "6.3 Describing Climate Normals\nQ5. What does each row in this dataset represent? How do you know?\n\n6.3.1 Creating a Basic Table for Descriptive Statistics\n\n## create a basic table of descriptive statistics for climate normals\nclimate_normals |&gt; st_drop_geometry() |&gt;\n  select(summer_temp) |&gt;\n  summarise(\n    n = n(),\n    num_na = sum(is.na(summer_temp)), \n    mean_temp = mean(summer_temp, na.rm = TRUE),\n    median_temp = median(summer_temp, na.rm = TRUE),\n    sd_temp = sd(summer_temp, na.rm = TRUE),\n    variance_temp = var(summer_temp, na.rm = TRUE),\n    mean_dev = mean(abs(summer_temp - mean(summer_temp, na.rm = TRUE)), na.rm = TRUE),\n    cv_temp = sd_temp / mean_temp,\n    min_temp = min(summer_temp, na.rm = TRUE),\n    max_temp = max(summer_temp, na.rm = TRUE),\n    skewness = skewness(summer_temp, na.rm = TRUE), \n    kurtosis = kurtosis(summer_temp, na.rm = TRUE)\n                               \n  ) |&gt; pivot_longer(everything(), names_to = \"Statistic\", values_to = \"Value\") |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"US Summer Temperatures (1991-2020)\",\n  ) %&gt;%\n  fmt_number(\n    columns = everything(),\n    decimals = 2\n  )\n\n\n\n\n\n\n\nUS Summer Temperatures (1991-2020)\n\n\nStatistic\nValue\n\n\n\n\nn\n469,758.00\n\n\nnum_na\n0.00\n\n\nmean_temp\n71.86\n\n\nmedian_temp\n71.55\n\n\nsd_temp\n7.64\n\n\nvariance_temp\n58.36\n\n\nmean_dev\n6.26\n\n\ncv_temp\n0.11\n\n\nmin_temp\n29.91\n\n\nmax_temp\n98.04\n\n\nskewness\n−0.09\n\n\nkurtosis\n−0.25\n\n\n\n\n\n\n\nQ6: Based on the descriptive statistics in the table, describe the overall distribution of summer temperatures across the U.S., including central tendency, variability, and evidence of skewness or outliers.\n\n\n6.3.2 Visualizing Descriptive Statistics\n\n6.3.2.1 Histogram\nA histogram is a graph that displays the frequency of observations within user-set “bins”. This can give us an understanding of the distribution of our data\n\n## create a histogram of summer temperatures\n\nggplot(climate_normals, aes(x = summer_temp)) +\n  geom_histogram(binwidth = 1, fill = \"lightgrey\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Frequency\"\n  ) \n\n\n\n\n\n\n\n\n\n\n6.3.2.2 Cumulative Frequency Plot\n\nggplot(climate_normals, aes(x = summer_temp)) + stat_ecdf() +\n  labs(\n    title = \"Cumulative Frequency Plot of Summer Temperatures (1991-2020)\",\n    x = \"Summer Temperature (°F)\",\n    y = \"Cumulative Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.3.2.3 Boxplot\nA boxplot is a standardized way of displaying the distribution of data based on a five-number summary: minimum, first quartile (Q1), median, third quartile (Q3), and maximum. It can also highlight outliers in the data.\n\nggplot(climate_normals, aes(y = summer_temp)) +\n  geom_boxplot(fill = \"lightgrey\") +\n  labs(\n    title = \"Boxplot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.3.2.4 Violin Plot\nA violin plot is a method of plotting numeric data and can be understood as a combination of a boxplot and a kernel density plot. It shows the distribution of the data across different values.\n\nggplot(climate_normals, aes(x = 1, y = summer_temp)) +\n  geom_violin(fill = \"lightgrey\", bw = 1.2)  + geom_boxplot(width=0.1) +\n  labs(\n    title = \"Violin Plot of Summer Temperatures (1991-2020)\",\n    y = \"Summer Temperature (°F)\"\n  ) + theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n\n\n\n\n\n\nQ7: What unique information does each of the four visualizations (histogram, cumulative frequency plot, boxplot, and violin plot) reveal about the distribution of summer temperatures?\n\n\n6.3.2.5 Map\n\ntm_shape(climate_normals) + tm_dots(\"summer_temp\")\n\n\n\n\n\n\n\n\nQ8: What does this map show us about the pattern of summer temperature across the U.S",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "describing_data.html#mini-challenge",
    "href": "describing_data.html#mini-challenge",
    "title": "6  Describing Data",
    "section": "6.4 Mini Challenge",
    "text": "6.4 Mini Challenge\nThis challenge will ask you to create some basic descriptive statistics for a single variable in the ACS census tract data.\n\nOpen the acs_tract_nc object. You can scroll over to see all the available variables. This file will tell you what each variable name means.\nSelect a variable that is interesting to you\nUsing the code above, as well as the Command Compendium, create a descriptive statistics table in a new code chunk\nUsing the code above, as well as the R Basics chapter, create at least one non-map data visualization\nUsing the code above, as well as the R Basics chapter, create a map of your variable\nAnswer the following questions:\n\nQ9. What did you learn about your variable?\nQ10. What did you learn about the spatial pattern of your variable?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Describing Data</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "9  Probability and Probability Distributions",
    "section": "",
    "text": "9.1 Reading in Data\nIn this chapter we will calculate empirical probability and apply discrete and continuous theoretical probability distributions to estimate probability (based on empirical estimates). We will also practice writing basic R commands for data manipulation. We will use two different probability distributions:\nBoth of these probability distributions are theoretical. A theoretical distribution is a mathematically defined model that describes how outcomes of a random process are expected to behave. We use them with empirical data because they let us approximate the underlying pattern, calculate probabilities for events we haven’t observed, and make predictions or inferences about the entire population, even from a limited sample.\nWe will use the following spatial datasets:\nTo follow along with this tutorial, make a new .Rmd document. As you move through the tutorial add chunks, headers, and relevant text to your document.\nPaste the following code into a chunk at the top of your document\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(stars)\nlibrary(gstat)\nlibrary(terra)\nlibrary(sp)\nlibrary(tigris)\n\n#read in rainfall data\nch_rainfall &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=1wYLHVVsemxQVjsdsmSzV9m0G6eLR_YK4\")\n  \n#read in pm data\npm_nc &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=19Axv5_bv7p4LbDWNDs4TmdFi3gfYeNXc\")\n\n#read in january data\nrdu_jan &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=1ABpiu5ABzAo-WXbYBp9LWgN8-lHuFQR9\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#calculating-empirical-probability-of-poor-air-quality",
    "href": "probability.html#calculating-empirical-probability-of-poor-air-quality",
    "title": "9  Probability and Probability Distributions",
    "section": "9.2 Calculating Empirical Probability of Poor Air Quality",
    "text": "9.2 Calculating Empirical Probability of Poor Air Quality\nEmpirical probability is the likelihood of an event based on data from an actual experiment or observational data. To calculate empirical probability, we can use the simple formula:\nP(E) (probability of an event) = f (number of times the event occurred) / n (total number of trials or observations)\nLet’s practice some basic data manipulation and calculate the empirical probability of a moderate or worse air quality at locations across North Carolina.\nQ1. What does each observation (each row) in the pm_nc object represent? How do you know?\nQ2. Based on Q1, what needs to happen to the data for us to be able to apply the empirical probability formula?\n\n# flag for moderate or worse (1 if moderate or worse 0 if not)\nflagged_pm &lt;- pm_nc |&gt; mutate(aq_flag = ifelse( pm_val &gt; 12, 1, 0))\n\nNow that we know, for each observation (date) at each location (lat, lon), whether or not the observation qualifies as an event, we can group the data to get the f (number of times the event occurred) and the n (total observations)\n\n#group by lat/lon point and summarize\ngrouped_pm &lt;- flagged_pm |&gt; group_by(lat, lon) |&gt; summarise(tot_flag = sum(aq_flag), tot_ob = n())\n\nQ3. Create a new object called pm_prob and calculate a new field that calculates the empirical probability of a moderate or worse air quality day at each lat/lon location. You will use the mutate command.\nQ4. Use this command grouped_pm_sf &lt;- YOUR_NEWOBJECT |&gt; st_as_sf(coords= c(\"lon\", \"lat\"), crs = 4326) to turn the data into a spatial object.\nQ5. Create a histogram of probability values. Then make a map of probability across the state.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#applying-the-binomial-distribution-to-chapel-hill-precipitation-data",
    "href": "probability.html#applying-the-binomial-distribution-to-chapel-hill-precipitation-data",
    "title": "9  Probability and Probability Distributions",
    "section": "9.3 Applying the Binomial Distribution to Chapel Hill Precipitation Data",
    "text": "9.3 Applying the Binomial Distribution to Chapel Hill Precipitation Data\nThe binomial distribution is a theoretical probability model that helps us predict how many times an event (a “success”) will happen in a fixed number of trials. The model assumes each trial is independent and the probability of success is the same each time. In reality, we often don’t know the true probability of an event. But we can get a good estimate of that probability using empirical data.\nIn this part of the tutorial, we will calculate the empirical probability of rainfall on a given day in Chapel Hill using daily data from 1891-2025 (note that this is a very simplified probability, given that trends differ by year and seasons) to answer questions like:\nOver 10 days in Chapel Hill, what’s the most likely number of rainy days?\nOver 10 days in Chapel Hill, what’s the probability that we have 0 rainy days?\nWe start by creating a new variable with a binary outcome (rain = 1, no rain = 0) and using basic R commands to calculate the empirical probability\n\n#create binary object\nch_rainfall_binary &lt;- ch_rainfall |&gt; mutate(rain = ifelse(prec_in &gt; 0, 1, 0))\n\n#calculate empirical probability\ntotal_days &lt;- length(ch_rainfall_binary$date)\ntotal_rainfall_days &lt;- sum(ch_rainfall_binary$rain)\n\np_rain &lt;- total_rainfall_days / total_days\n\nNow we know that the empirical probability of any amount of rain on a given day is .337, or 33.7%. Using this probability, we can apply the binomial distribution and plot the results.\n\nprobability_days &lt;- tibble(\n  num_days = 0:10,\n  #here is the main probability function\n  probability = dbinom(x = 0:10, size = 10, prob = p_rain)\n)\n\n#plotting probability by day\nggplot(probability_days, aes(x = num_days, y = probability)) +\n  geom_line() +  scale_x_continuous(breaks = 0:10)\n\n\n\n\n\n\n\n\nQ6. Based on the outcome of the binomial distribution, what are the answers to the questions above?\nWe can also use the binomial distribution to calculate cumulative probabilities. For instance, if we wanted to know the probability that we would have 3 days or less, we could calculate that probability by using the pbinom function (which calculates cumulative probability), instead of the dbinom function\n\nless3_days &lt;- pbinom(3, 10, p = p_rain)\n\nTo calculate the probability of having more than 3 days we could calculate it by taking the complement\n\nmore3_days &lt;- 1 - less3_days",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#applying-the-geometric-distribution-to-chapel-hill-precipitation-data",
    "href": "probability.html#applying-the-geometric-distribution-to-chapel-hill-precipitation-data",
    "title": "9  Probability and Probability Distributions",
    "section": "9.4 Applying the Geometric Distribution to Chapel Hill Precipitation Data",
    "text": "9.4 Applying the Geometric Distribution to Chapel Hill Precipitation Data\nLike the Binomial distribution, the Geometric distribution is also a theoretical probability model. We can use the Binomial distribution to estimate the probability of how many trials it would take to achieve a success. We could use the Binomial distribution to ask a question like:\nWhat is the probability that it will take 5 days to get the first day with precipitation?\n\n#turn into a dataframe\nprobability_days_geom &lt;- tibble(\n  num_days = 1:10,\n  #main probability function is dgeom\n  probability = dgeom(x = num_days - 1, prob = p_rain)\n)\n\n#plotting probability by day\nggplot(probability_days_geom, aes(x = num_days, y = probability)) +\n  geom_line() +  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\nQ7. Based on this probability distribution, what is the likelihood that it takes 5 days to see the first rainfall?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#mini-challenge",
    "href": "probability.html#mini-challenge",
    "title": "9  Probability and Probability Distributions",
    "section": "9.4 Mini Challenge",
    "text": "9.4 Mini Challenge\nUsing the script above, use the binomial distribution and geometric distribution to answer the following questions:\n\nOver 10 days in Chapel Hill, what’s the most likely number of days with rain over .5 inches?\nWhat is the cumulative probability that Chapel Hill will get at least 3 days of rain over .5 inches in the next 10 days?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#the-normal-distribution",
    "href": "probability.html#the-normal-distribution",
    "title": "9  Probability and Probability Distributions",
    "section": "9.5 The Normal Distribution",
    "text": "9.5 The Normal Distribution\nThe normal distribution is a theoretical model that describes how continuous data tend to cluster around an average, forming a smooth, symmetric, bell-shaped curve. Although no real dataset is perfectly normal, many natural variables behave approximately this way because they are influenced by many small, independent factors. January temperatures at a specific location (in our case, RDU airport) are a good example: most days fall near the typical mid-winter temperature, with fewer extremely warm or cold days. By estimating the mean and standard deviation from actual January data, we can use the normal distribution to model and predict temperature patterns.\nWhen we plot January daily maximum temperatures, we can see that the output is approximately normal (in a vague sense).\n\n#density plot of max_temp variable\nggplot(rdu_jan, aes(x = max_temp)) + geom_density()\n\n\n\n\n\n\n\n\nIf the data was perfectly normal, the distribution would look like this:\n\n#create normal distribution\njan_norm &lt;- tibble(\n  temp = seq(min(rdu_jan$max_temp), max(rdu_jan$max_temp)),\n  density = dnorm(temp, mean(rdu_jan$max_temp), sd(rdu_jan$max_temp))\n)\n#plot normal distribution\nggplot(jan_norm, aes(x = temp, y = density)) +\n  geom_line() \n\n\n\n\n\n\n\n\nA Q–Q plot is a common tool for assessing whether a dataset follows a normal distribution. In a Q–Q plot, data that are normally distributed should fall roughly along the reference line. In our case, the points align reasonably well in the center of the distribution, but the tails show clear deviations from the line. This indicates that while the bulk of the data is approximately normal, the extreme values do not follow the normal distribution as closely.\nBecause of this, using the normal distribution may still provide reasonable approximations for probabilities near the center of the distribution, but estimates involving the tails (especially rare events) should be interpreted with caution. Tail deviations suggest that the normal distribution may underestimate or overestimate the likelihood of extreme outcomes, so empirical or alternative distributional methods may be more appropriate for modeling rare events.\n\nqqnorm(rdu_jan$max_temp, pch = 1, frame = FALSE)\nqqline(rdu_jan$max_temp, col = \"steelblue\", lwd = 2)\n\n\n\n\n\n\n\n\nTo apply the normal distribution, we could calculate the probability that a day in January has a maximum temperature of 60 or above by calculating the cumulative density function.\n\n#pnorm calculates the CDF up to the value. So we need to do 1-pnorm to get the probability above that value\nprob &lt;- 1 - pnorm(60, mean =  mean(rdu_jan$max_temp), sd = sd(rdu_jan$max_temp))\n\nThis shows us that there is a 22.4% chance that a day in January at RDU will have a maximum be above 60 degrees.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#mini-challenge-1",
    "href": "probability.html#mini-challenge-1",
    "title": "9  Probability and Probability Distributions",
    "section": "9.6 Mini Challenge",
    "text": "9.6 Mini Challenge\nUsing the code above, calculate the probability that a day has a maximum temperature of 83 degrees or above using the normal distribution. Then calculate the empirical probability using the observed values. Discuss the following questions:\n\nHow do the theoretical and empirical probabilities differ?\nWhy might this be the case?\nWhat are potential problems in applying the normal distribution to estimate rare events in this dataset?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-mapping",
    "href": "probability.html#probability-mapping",
    "title": "9  Probability and Probability Distributions",
    "section": "9.8 Probability Mapping",
    "text": "9.8 Probability Mapping\nFor spatial data, we can often calculate how probabilities of an event differ across space. For instance, our empirical probability of a moderate or worse air quality day at locations across North Carolina. We can map these probabilities:\n\npm_probs &lt;- grouped_pm |&gt; mutate(p = tot_flag / tot_ob)\ngrouped_pm_sf &lt;- pm_probs |&gt; st_as_sf(coords= c(\"lat\", \"lon\"), crs = 4326)\n\ntm_shape(grouped_pm_sf) + tm_dots(fill = \"p\")\n\n\n\n\n\n\n\n\nAlready, from this map, we can start to interpret spatial patterns in the probability of having moderate or worse air quality day.\nQ8. What patterns do you see?\nThese probabilities are currently shown only at discrete locations (points). However, in many cases, it’s useful to create a continuous probability surface to better visualize spatial trends and to estimate values at unsampled locations. One common method to do this is Inverse Distance Weighting (IDW). IDW assumes that points closer together are more similar than points farther apart, so it interpolates values between your measured points based on distance. Essentially, each unsampled location is assigned a weighted average of nearby points, with closer points contributing more to the estimate. Using IDW, you can generate a smooth surface showing the probability of moderate or worse air quality across the entire region, rather than just at individual points. This makes it easier to identify hotspots and spatial gradients.\n\n#reproject pm data\nreprojected_data &lt;- grouped_pm_sf |&gt; st_transform(crs = 2264)\n\n#turn pm data into sp\npm_sp &lt;- as(reprojected_data, 'Spatial')\n\n#get north carolina boundary to create grid\nnc_boundary &lt;- states(cb = T, progress_bar = FALSE) |&gt; filter(NAME == \"North Carolina\") %&gt;% st_transform(2264)\n\n#turn into an sp object\nnc_sp &lt;-  as(nc_boundary, 'Spatial')\n\n#create grid for the raster\ngrd &lt;- as.data.frame(spsample(nc_sp, \"regular\", n=50000))\n\n#sets parameters for the grid\nnames(grd)       &lt;- c(\"X\", \"Y\")\ncoordinates(grd) &lt;- c(\"X\", \"Y\")\ngridded(grd)     &lt;- TRUE  # Create SpatialPixel object\nfullgrid(grd)    &lt;- TRUE  # Create SpatialGrid object\n\n#sets crs\ncrs(grd) &lt;- crs(pm_sp)\n\n#does the interpolation. idp sets sets the influence of points based on distance: higher values = stronger influence of nearby points, weaker influence of distant points.\np.idw &lt;- idw(p ~ 1, pm_sp, newdata=grd, idp = 4)\n\n[inverse distance weighted interpolation]\n\n#rasterize the grid\nr       &lt;- rast(p.idw)\nr.m     &lt;- mask(r, nc_boundary)\n\n#create map\ntm_shape(r.m[\"var1.pred\"]) + \n  tm_raster(col.scale = tm_scale_intervals(values = \"BuRd\", n = 8),\n            col.legend = tm_legend(position = tm_pos_out(), \n                                   title = \"Daily Probability\" ))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "clt.html",
    "href": "clt.html",
    "title": "10  Central Limit Theorem",
    "section": "",
    "text": "10.1 Reading in Data\nThe Central Limit Theorem states that if a sample size (n) is large enough, the sampling distribution of the sample mean will be approximately normal, regardless of the shape of the population distribution. In general, a sample size of n &gt; 30 is considered to be large enough for the Central Limit Theorem to hold.\nBecause of the CLT, we can make inferences about a population using samples, even if the original population’s distribution is unknown or not normal.\nIn this chapter, we will demonstrate the CLT using a raster of tree canopy coverage across the Chapel Hill Area (from the 2023 National Land Cover Dataset).\nTo follow along with this tutorial, make a new .Rmd document. As you move through the tutorial add chunks, headers, and relevant text to your document.\nPaste the following code into a chunk at the top of your document\n#load libraries\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\n\n#read in canopy raster\ncanopy_raster &lt;- rast(\"https://drive.google.com/uc?export=download&id=1qxedXO256Xf-dP-rSeE0icQ-Hi-a3ioC\")\nThis data represents the percent of tree cover per 30-meter pixel. When we map the data, we can see the spatial pattern in tree canopy coverage across the Chapel Hill area\n#map canopy raster \ntm_shape(canopy_raster) + \ntm_raster(col.scale = tm_scale_intervals(values = \"carto.sunset_dark\", n = 5))\nQ1: Can you pick out any known features from this map?\nBecause we have the entire raster, we can calculate the true mean of this raster and visualize the population distribution.\n#turn raster into a dataframe\ncanopy_raster_df &lt;- as.data.frame(canopy_raster, xy = TRUE)\n\n#calculate mean and sd\ntree_coverage_mean &lt;- mean(canopy_raster_df$NLCD_Percent_Tree_Canopy_Cover)\ntree_coverage_sd &lt;- sd(canopy_raster_df$NLCD_Percent_Tree_Canopy_Cover)\n\n#density plot\nggplot(canopy_raster_df, aes(x = NLCD_Percent_Tree_Canopy_Cover)) + geom_histogram(binwidth = 3)\nQ2: How can we interpret that mean value? What does the value represent?\nQ3: What are the characteristics of the distribution of tree canopy cover across Chapel Hill?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "clt.html#reading-in-data",
    "href": "clt.html#reading-in-data",
    "title": "10  Central Limit Theorem",
    "section": "",
    "text": "10.1.1 CLT Part 1\nRegardless of the population’s shape, the sampling distribution of the sample mean becomes approximately normal as the sample size increases\nTo test the CLT, we can take simple random samples of our raster. Let’s take 300 samples, with n = 75.\n\n#parameters for sampling\nsample_size &lt;- 75\nnum_samples &lt;- 300\n\n#draw random samples\nsamples &lt;- replicate(num_samples, sample(canopy_raster_df$NLCD_Percent_Tree_Canopy_Cover, size = sample_size,replace = TRUE))\n\n#calculate sample means\nsample_means &lt;- colMeans(samples)\n\nsample_means_df &lt;- data.frame(sample_means = sample_means)\n\nmean_of_means &lt;- mean(sample_means_df$sample_means)\n\n#density plot\nggplot(sample_means_df, aes(x = sample_means)) + geom_histogram(binwidth = 3)\n\n\n\n\n\n\n\n\nQ4: Run the code above with a smaller sample size (n = 5). Does the distribution of means still follow a normal distribution?\n\n\n10.1.2 CLT Part 2\nThe mean of the sampling distribution of the sample mean will become the population mean\nQ5: Calculate the mean of our sample means. How does the mean compare to the population mean?\n\n\n10.1.3 CLT Part 3\nThe standard deviation of the sampling distribution is called the standard error, and is related to the population standard deviation by the formula:\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\]\nwhere \\({s}\\) is equal to the sample standard deviation, and \\({n}\\) is the sample size.\nA smaller SE means the sample mean is likely closer to the population mean, so our estimate of the mean is more accurate and precise.\nWe can test this by running repeated samples at different sample sizes\n\nsample_sizes &lt;- c(5, 10, 50, 100, 200)  # small and large sample\nnum_samples &lt;- 300\n\nse_values &lt;- data.frame(sample_size = integer(), SE = numeric())\n\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(num_samples, \n                            mean(sample(canopy_raster_df$NLCD_Percent_Tree_Canopy_Cover, \n                                        size = n, replace = TRUE)))\n  se_values &lt;- rbind(se_values, data.frame(sample_size = n, SE = sd(sample_means)))\n}\n\n# Plot SE vs sample size\nggplot(se_values, aes(x = sample_size, y = SE)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"Standard Error vs Sample Size\",\n       x = \"Sample Size (n)\",\n       y = \"Standard Error (SE)\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "estimation_sampling.html",
    "href": "estimation_sampling.html",
    "title": "11  Estimation in Sampling",
    "section": "",
    "text": "11.1 Reading in Data\nBecause we know from the CLT that repeated sampling of a population will result in a normal distribution of means, we can use the properties of the normal distribution to estimate population parameters from a sample.\nThere are two types of sample estimations:\nWe will use the following dataset:\nPUMS data are individual-level, anonymized records from the American Community Survey. They represent a sample of the population and serve as the raw data used to produce ACS summary statistics at aggregated geographic levels, such as block groups and tracts.\nFor the purposes of calculating confidence intervals, we will treat the PUMS sample as if it were a simple random sample. In reality, PUMS is a complex survey sample with weights and clustering, so the independence assumption of the CLT is not strictly met.\nThe variables in this dataset are:\nTo follow along with this tutorial, make a new .Rmd document. As you move through the tutorial add chunks, headers, and relevant text to your document.\nlibrary(tidyverse)\nlibrary(tmap)\n\n#read in pums data\nch_pums &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=1ifsUSH8veyhn8x8gUMo0MGu4VxYWpI9p\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimation in Sampling</span>"
    ]
  },
  {
    "objectID": "estimation_sampling.html#calculating-point-estimate-and-confidence-interval-for-means",
    "href": "estimation_sampling.html#calculating-point-estimate-and-confidence-interval-for-means",
    "title": "11  Estimation in Sampling",
    "section": "11.2 Calculating Point Estimate and Confidence Interval for Means",
    "text": "11.2 Calculating Point Estimate and Confidence Interval for Means\nTo calculate a confidence interval for a mean, we use this formula\n\\[\n\\bar{x} \\pm t^* \\frac{s}{\\sqrt{n}}\n\\]\nWhere \\(\\bar{x}\\) is equal to the sample mean (or the point estimate), \\(t^*\\) is the critical t-value, \\({s}\\) is equal to the sample standard deviation, and \\({n}\\) is the sample size. We use t distribution for means because we must estimate the population standard deviation and that extra uncertainty makes the sampling distribution wider than the normal curve.\nWe can “manually” calculate the confidence interval in R.\nThe script below shows the calculation of the 95% confidence interval (because we’re running a two-tail interval, the critical value is .975 for each side) for the average commute to work for the Chapel Hill population.\n\n#calculate mean\nsample_mean &lt;- mean(ch_pums$travel_time_work)\n\n#calculate sample variance\nsample_variance &lt;- sd(ch_pums$travel_time_work)**2\n\n#sample size\nsample_size &lt;- nrow(ch_pums)\n\n#command to get critical value\nt_val &lt;- qt(0.975, df = sample_size - 1)\n\n#calculate lower bound\nconfidence_l &lt;- sample_mean - t_val * (sqrt(sample_variance/sample_size))\n#calculate higher bound\nconfidence_h &lt;- sample_mean + t_val * (sqrt(sample_variance/sample_size))\n\nFrom running this script, we learn that the point estimate of the population mean (which is equal to the sample mean) is 11.845. Applying the confidence interval calculation, we know that there is a 95% chance that the true population mean is between 11.42 and 12.27 (or 11.845 +/- .425).\nThere is also an easy command to calculate point estimates and confidence intervals for means\n\n#t test command (assumes two-tail)\nt.test(ch_pums$travel_time_work, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  ch_pums$travel_time_work\nt = 54.606, df = 6501, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.42019 12.27068\nsample estimates:\nmean of x \n 11.84543 \n\n\nQ1. We have a large sample size (6502). What would happen to the confidence interval if we had a smaller sample? You can try it out by artificially changing the sample size in the code above\nQ2. Rerun the script above using a difference confidence level? How do the results change?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimation in Sampling</span>"
    ]
  },
  {
    "objectID": "estimation_sampling.html#calculating-point-estimate-and-confidence-interval-for-proportions",
    "href": "estimation_sampling.html#calculating-point-estimate-and-confidence-interval-for-proportions",
    "title": "11  Estimation in Sampling",
    "section": "11.3 Calculating Point Estimate and Confidence Interval for Proportions",
    "text": "11.3 Calculating Point Estimate and Confidence Interval for Proportions\nWe use the following formula to calculate a confidence interval for a proportion.\n\\[\n\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWhere \\(\\hat{p}\\) is the sample proportion (point estimate), \\(z^*\\) is the critical z-value, and \\(n\\) is the sample size. For proportions, the variability is already built into the binomial model, so we do not need to estimate the standard deviation, which is why we can use the z distribution\nWe can “manually” calculate the confidence interval in R.\nThe script below shows the calculation of the 95% confidence interval (because we’re running a two-tail interval, the critical value is .975 for each side) for the proportion of people who have public health insurance\n\n#people with public health insurance (we can take the sum since it is represented as 1/0 binary)\ntotal_pop_pub_health &lt;- sum(ch_pums$public_health_ins)\ntotal_sample_pop &lt;- nrow(ch_pums)\n\nprop_pub_health &lt;- total_pop_pub_health/ total_sample_pop\n\n#get z critical value for two tail confidence 95%\nz_val &lt;- qnorm(0.975)\n\n#lower bound of confidence interval\nconfidence_lower &lt;- prop_pub_health - z_val * (sqrt(prop_pub_health * (1 -prop_pub_health)/ total_sample_pop))\n\n#higher bound of confidence interval\nconfidence_higher &lt;- prop_pub_health + z_val * (sqrt(prop_pub_health * (1 -prop_pub_health)/ total_sample_pop))\n\nFrom the code above, we can say that the best point estimate for the proportion of people with public health insurance in Chapel Hill is .172 and that we are 95% confident that the true proportion falls between .163 and .182.\nThere is also an easy command to calculate point estimates and confidence intervals for proportions\n\n# calculate CI using Wilson method\nprop.test(total_pop_pub_health, total_sample_pop, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  total_pop_pub_health out of total_sample_pop, null probability 0.5\nX-squared = 2789.8, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.1633459 0.1818612\nsample estimates:\n        p \n0.1724085 \n\n\nYou should notice a small difference between the CI that we manually calculated and the CI that was automatically calculated. The built-in prop.test command in R uses a slightly different calculation, called the Wilson interval, which adjusts both the center and the width of the interval to account for the fact that proportions are bounded between 0 and 1, making it more accurate than the Wald method for many sample sizes. Our manual calculation uses the Wald (textbook) method, which is simpler but can underestimate or overestimate the true confidence interval in some cases.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimation in Sampling</span>"
    ]
  },
  {
    "objectID": "estimation_sampling.html#mini-challenge",
    "href": "estimation_sampling.html#mini-challenge",
    "title": "11  Estimation in Sampling",
    "section": "11.4 Mini Challenge",
    "text": "11.4 Mini Challenge\nUsing the code above, calculate the point estimate and the 90% and 95% confidence interval for the two other variables in the pums dataset (hrs_wrked_per_weeek and income)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Estimation in Sampling</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-5",
    "href": "labs.html#lab-5",
    "title": "14  Labs",
    "section": "14.4 Lab 5",
    "text": "14.4 Lab 5\n\n14.4.1 Overview\nIn the Estimation with Sampling tutorial we learned about how to make point and interval estimations of means and proportions from samples of a population.\nIn this lab, you will practice calculating point and interval estimations from a simulated simple random sample generated from the Behavioral Risk Factor Surveillance System 2023 data. This sample represents the population of the United States and explores various health-related behaviors and outcomes. Each row represents an individual’s responses to the survey.\nThe variables in the dataset are:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nphyshlth\nNumber of days of poor physical health in the last 30 days\n\n\nmenthlth\nNumber of days of poor mental health in the last 30 days\n\n\nx.state\nstate FIPS code\n\n\nexerany2\nAny exercise in the last 30 days (1= yes, 0= no)\n\n\n\nRemember that you can use the Command Compendium to help you modify R commands.\n\n\n14.4.2 Specifications\nThis lab is designed to assess the Concept 5 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nHTML and RMD versions the R Markdown file have been submitted.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output) so that the analysis is easy to read and understand.\n\n\nDescriptive Statistics: Student creates a descriptive statistics table and non-map graphic for the three variables. Written interpretation describes distribution and demonstrates understanding of the results.\n\n\nCountrywide Estimates: Student calculates a point estimate and confidence interval for the three variables of interest for the entire country. Written interpretation describes the estimates and relates the estimates to conclusions about the full population.\n\n\nStatewide Estimates: Student calculates a point estimate and confidence interval for the three variables of interest for each state and maps the results. Written interpretation describes variability between states in terms of estimates and confidence interval sizes, as well as spatial patterns.\n\n\n\n\n\n14.4.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab5.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n#load libraries\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(tigris)\n\n#BRFSS sample from 2023\nbrfss_sample &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=11rKXmplEk0dcvi7lzEQ0OUrXDrnvK4MR\") |&gt; mutate(x.state = str_pad(as.character(x.state), width = 2, side = \"left\", pad = \"0\"))\n\nAdd a third-level header called “Descriptive Statistics” and a new code chunk below the header. In this code chunk you should:\n\nCreate a descriptive statistics table each of the three variables (not including state) in the dataset.\nCreate one non-map graphic for each of the three variables.\n\nBelow this code chunk, write a few sentences the characteristics of the sample distribution for each of the variables.\nAdd a third-level header called “Countrywide Estimates” and add a new code chunk below the header. In this code chunk you should:\n\nCalculate (at a 95% confidence level) the point estimation and confidence interval for each of the three variables.\n\nBelow this code chunk, write a few sentences that describe the values and what this tells us about the values of the population.\nAdd a third-level header called “Statewide Estimates and Mapping” and add a new code chunk below the header. In this code chunk you should:\n\nCalculate (at a 95% confidence level) the point estimation and confidence interval for each of the three variables for each state (there will be two missing states). Also calculate the size of the confidence interval for each state\nAdd the following code to join your dataset to a spatial boundary file\n\n#get just continental US for mapping purposes\nstate_boundaries &lt;- states() |&gt; filter(!(GEOID %in% c(\"15\", \"02\",\"60\", \"66\", \"69\", \"72\", \"78\")))\n\n#add state data to state boundaries\nspatial_data &lt;- state_boundaries |&gt; left_join(YOUR_AGGREGATED_STATE_DATASET, join_by(\"GEOID\" == \"x.state\"))\n\nCreate a map of one of the variables. Also create a map of the size of the confidence interval per state\n\nBelow this code chunk, write a few sentence that compare the state estimates and the size of their confidence intervals. Explain why some states might have smaller or larger intervals. Also describe any spatial patterns that you see in the means or the size of the confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "command.html#probability",
    "href": "command.html#probability",
    "title": "5  Command Compendium",
    "section": "5.5 Probability",
    "text": "5.5 Probability\n\n5.5.1 Binomial Distribution\n\n#per trial probability, x = number of trials, p = probability per trial\nprobability_per_trial &lt;- tibble(\n  num_trials = 0:X,\n  #here is the main probability function\n  probability = dbinom(x = 0:X, size = X, prob = p)\n)\n\n#cumulative probability\ncum_prob &lt;- pbinom(VALUELESSTHAN, TOTALTRIALS, p = p)\n\n\n\n5.5.2 Normal Distribution\n\n#pnorm calculates the CDF up to the value. Do 1-pnorm to get the probability above that value\nprob &lt;- pnorm(VALUEOFINTEREST, mean =  mean(DATASET$VARIABLE), sd = sd(DATASET$VARIABLE))\n\n\n\n5.5.3 Probability Map\n\n#reproject data \nreprojected_data &lt;- DATASET |&gt; st_transform(crs = 2264)\n\n#turn pm data into sp\npm_sp &lt;- as(reprojected_data, 'Spatial')\n\n#get north carolina boundary to create grid\nnc_boundary &lt;- states(cb = T, progress_bar = FALSE) |&gt; filter(NAME == \"North Carolina\") %&gt;% st_transform(2264)\n\n#turn into an sp object\nnc_sp &lt;-  as(nc_boundary, 'Spatial')\n\n#create grid for the raster\ngrd &lt;- as.data.frame(spsample(nc_sp, \"regular\", n=50000))\n\n#sets parameters for the grid\nnames(grd)       &lt;- c(\"X\", \"Y\")\ncoordinates(grd) &lt;- c(\"X\", \"Y\")\ngridded(grd)     &lt;- TRUE  # Create SpatialPixel object\nfullgrid(grd)    &lt;- TRUE  # Create SpatialGrid object\n\n#sets crs\ncrs(grd) &lt;- crs(pm_sp)\n\n#does the interpolation. idp sets sets the influence of points based on distance: higher values = stronger influence of nearby points, weaker influence of distant points.\np.idw &lt;- idw(p ~ 1, pm_sp, newdata=grd, idp = 4)\n\n#rasterize the grid\nr       &lt;- rast(p.idw)\nr.m     &lt;- mask(r, nc_boundary)\n\n#create map\ntm_shape(r.m[\"var1.pred\"]) + \n  tm_raster(col.scale = tm_scale_intervals(values = \"BuRd\", n = 8),\n            col.legend = tm_legend(position = tm_pos_out(), \n                                   title = \"TITLE\" ))\n\n\n\n5.5.4 Point Estimate and CI for Mean\n\n#t test command (assumes two-tail)\nt.test(DATASET$VARIABLE, conf.level = 0.95)\n\n\n\n5.5.5 Point Estimate and CI for Proportion\n\nprop.test(TOTALSUCCESSES, n, conf.level = 0.95)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Command Compendium</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-6",
    "href": "labs.html#lab-6",
    "title": "12  Labs",
    "section": "12.4 Lab 6",
    "text": "12.4 Lab 6\n\n12.4.1 Overview\nIn the Estimation with Sampling tutorial we learned about how to make point and interval estimations of means and proportions from samples of a population.\nIn this lab, you will practice calculating point and interval estimations from a simulated simple random sample generated from the Behavioral Risk Factor Surveillance System 2023 data. This sample represents the population of the United States and explores various health-related behaviors and outcomes. Each row represents an individual’s responses to the survey.\nThe variables in the dataset are:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nphyshlth\nNumber of days of poor physical health in the last 30 days\n\n\nmenthlth\nNumber of days of poor mental health in the last 30 days\n\n\nx.state\nstate FIPS code\n\n\nexerany2\nAny exercise in the last 30 days (1= yes, 0= no)\n\n\n\nRemember that you can use the Command Compendium to help you modify R commands.\n\n\n12.4.2 Specifications\nThis lab is designed to assess the Concept 4 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nHTML and RMD versions the R Markdown file have been submitted.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output) so that the analysis is easy to read and understand.\n\n\nDescriptive Statistics: Student creates a descriptive statistics table and non-map graphic for the three variables. Written interpretation describes distribution and demonstrates understanding of the results.\n\n\nCountrywide Estimates: Student calculates a point estimate and confidence interval for the three variables of interest for the entire country. Written interpretation describes the estimates and relates the estimates to conclusions about the full population.\n\n\nStatewide Estimates: Student calculates a point estimate and confidence interval for the three variables of interest for each state and maps the results. Written interpretation describes variability between states in terms of estimates and confidence interval sizes, as well as spatial patterns.\n\n\n\n\n\n12.4.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab6.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n#load libraries\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(tigris)\n\n#BRFSS sample from 2023\nbrfss_sample &lt;- read_csv(\"https://drive.google.com/uc?export=download&id=11rKXmplEk0dcvi7lzEQ0OUrXDrnvK4MR\") |&gt; mutate(x.state = str_pad(as.character(x.state), width = 2, side = \"left\", pad = \"0\"))\n\nAdd a third-level header called “Descriptive Statistics” and a new code chunk below the header. In this code chunk you should:\n\nCreate a descriptive statistics table each of the three variables (not including state) in the dataset.\nCreate one non-map graphic for each of the three variables.\n\nBelow this code chunk, write a few sentences the characteristics of the sample distribution for each of the variables.\nAdd a third-level header called “Countrywide Estimates” and add a new code chunk below the header. In this code chunk you should:\n\nCalculate (at a 95% confidence level) the point estimation and confidence interval for each of the three variables.\n\nBelow this code chunk, write a few sentences that describe the values and what this tells us about the values of the population.\nAdd a third-level header called “Statewide Estimates and Mapping” and add a new code chunk below the header. In this code chunk you should:\n\nCalculate (at a 95% confidence level) the point estimation and confidence interval for each of the three variables for each state (there will be two missing states). Also calculate the size of the confidence interval for each state\nAdd the following code to join your dataset to a spatial boundary file\n\n#get just continental US for mapping purposes\nstate_boundaries &lt;- states() |&gt; filter(!(GEOID %in% c(\"15\", \"02\",\"60\", \"66\", \"69\", \"72\", \"78\")))\n\n#add state data to state boundaries\nspatial_data &lt;- state_boundaries |&gt; left_join(YOUR_AGGREGATED_STATE_DATASET, join_by(\"GEOID\" == \"x.state\"))\n\nCreate a map of one of the variables. Also create a map of the size of the confidence interval per state\n\nBelow this code chunk, write a few sentence that compare the state estimates and the size of their confidence intervals. Explain why some states might have smaller or larger intervals. Also describe any spatial patterns that you see in the means or the size of the confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html",
    "href": "autocorrelation.html",
    "title": "13  Spatial Autocorrelation",
    "section": "",
    "text": "13.1 Reading in Data\nTobler’s First Law of Geography reminds us that everything is related to everything else, but near things are more related than distant things. Spatial autocorrelation describes how a variable is correlated with itself across geographic space. In other words, spatial autocorrelation can tell us how related near things are to each other in our dataset.\nQuantifying autocorrelation is useful because it helps us understand the spatial pattern/structure of our data, which helps us in generating hypotheses about the processes that generate the pattern. In addition, spatial autocorrelation allows us to understand how spatially dependent our variable is. Spatial dependence can be a serious problem for applying traditional statistical methods because these methods generally have an assumption of independence. Measuring the strength of spatial dependence helps us evaluate how serious this issue is and decide whether we need to adjust our approach- for example, by modifying our sampling design, using methods that account for spatial structure, or interpreting results with greater caution.\nIn this chapter, we will explore autocorrelation in the Median Household Income variable from the 2023 American Community Survey for North Carolina census tracts\nTo follow along with this tutorial, make a new .Rmd document. As you move through the tutorial add chunks, headers, and relevant text to your document.\nFor this tutorial, you will need to manually download the data and put it in the same folder that your file is saved in. The rgeoda package won’t work with streamed data.\nlibrary(rgeoda)\nlibrary(tmap)\nlibrary(tidyverse)\nlibrary(spdep)\nlibrary(sf)\n\n#read in the census tracts dataset using geoda and sf.\n#the sf object is necessary for mapping. Note that the crs \n#of the dataset is NAD83/ North Carolina, meaning that the units are feet\ntracts &lt;- geoda_open(\"clust_acs.shp\")\ntracts_sf &lt;- st_read(\"clust_acs.shp\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#defining-neighbors",
    "href": "autocorrelation.html#defining-neighbors",
    "title": "13  Spatial Autocorrelation",
    "section": "13.2 Defining Neighbors",
    "text": "13.2 Defining Neighbors\nIn order to quantify spatial autocorrelation, we need to define each observation’s “spatial neighborhood” so that we can see how related an observation is to the observations near it. There are many ways to define neighbors, and the decision about what definition to use should reflect the processes you hypothesize might be shaping the spatial pattern. For example, if you are studying the spread of a disease, it may be appropriate to define neighbors based on travel networks or adjacency of regions. If you are examining soil properties, a distance-based neighborhood might better reflect how conditions diffuse across space.\nWe can explore two different neighborhood definitions using our ACS dataset:\n\n13.2.1 Distance Threshold\nOne way to determine neighbors is to set a distance threshold. Any observations within a set distance would be considered a neighbor. For our census tracts, we could set a distance of 3 miles. The code below demonstrates the neighborhood for the census tract that campus is in.\n\n#isolate ch tract\nch_tract &lt;- tracts_sf |&gt; filter(GEOID == 37135011400)\n\n#get 10 mile radius\nneighborhood_rad &lt;- ch_tract |&gt; st_buffer(15840)\n\n#get neighbors\nneighbors &lt;- st_filter(tracts_sf, neighborhood_rad)\ntm_shape(neighbors) + tm_polygons() + tm_shape(ch_tract) + tm_polygons(fill = \"blue\")\n\n\n\n\n\n\n\n\n\n\n13.2.2 Queens Case\nThe Queens Case neighborhood definition is extremely common in spatial statistics that consider contiguous aerial units (like census tracts). The Queens neighborhood selects neighbors based on any contact (including corners) with the feature of interest. The Queens neighborhood for Chapel Hill would look this this:\n\nqueen_neighbors &lt;- tracts_sf |&gt; st_filter(ch_tract, .predicates = st_intersects) \n\ntm_shape(queen_neighbors) + tm_polygons() + tm_shape(ch_tract) + tm_polygons(fill = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#calculating-global-autocorrelation",
    "href": "autocorrelation.html#calculating-global-autocorrelation",
    "title": "13  Spatial Autocorrelation",
    "section": "13.3 Calculating Global Autocorrelation",
    "text": "13.3 Calculating Global Autocorrelation\nMoran’s I gives an indication of how clustered, random, or dispersed a dataset value is across space. It goes from -1 (perfect dispersion) to 1 (perfect clustering). Let’s start by making a basic map of our variable of interest\n\ntm_shape(tracts_sf) + tm_polygons(fill = \"med_hh_inc\", fill.scale = tm_scale_intervals(style = \"fisher\"), col_alpha = 0)\n\n\n\n\n\n\n\n\nQ1: Just by looking at the values on the map, do you see evidence of a spatial pattern?\nWe can formalize this by calculating Moran’s I using Queens Case neighbors\n\nnb &lt;- poly2nb(tracts_sf, queen = TRUE) # queen shares point or border\nnbw &lt;- nb2listw(nb, style = \"W\")\n\ngmoran &lt;- moran.test(tracts_sf$med_hh_inc, nbw)\ngmoran\n\n\n    Moran I test under randomisation\n\ndata:  tracts_sf$med_hh_inc  \nweights: nbw    \n\nMoran I statistic standard deviate = 51.74, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5886766588     -0.0003806624      0.0001296167 \n\n\nOur Moran’s I indicates strong clustering (.59) at a highly statistically significant level (very small p-value). Therefore, we now know that, at the global scale, this dataset is clustered.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#local-indicators-of-spatial-autocorrelation-lisa",
    "href": "autocorrelation.html#local-indicators-of-spatial-autocorrelation-lisa",
    "title": "13  Spatial Autocorrelation",
    "section": "13.4 Local Indicators of Spatial Autocorrelation (LISA)",
    "text": "13.4 Local Indicators of Spatial Autocorrelation (LISA)\nWhile we know that the data is clustered because of our Moran’s I value, we don’t know where, specifically, these clusters of high and low income are. Local Indicators of Spatial Autocorrelation quantify where statistically significant clusters exist in a dataset\n\nlocali&lt;-localmoran_perm(tracts_sf$med_hh_inc, nbw) %&gt;%\n  as_tibble() %&gt;%\n  set_names(c(\"local_i\", \"exp_i\", \"var_i\", \"z_i\", \"p_i\",\n              \"p_i_sim\", \"pi_sim_folded\", \"skewness\", \"kurtosis\"))\n\n#bind LISA results\ntracts_sf &lt;- tracts_sf %&gt;%\n  bind_cols(locali)\n\n#Divide into quadrants\ntracts_sf &lt;- tracts_sf %&gt;%\n  mutate(incz =  as.numeric(scale(med_hh_inc)),\n         evratezlag = lag.listw(nbw, incz),\n         lisa_cluster = case_when(\n           p_i &gt;= 0.05 ~ \"Not significant\",\n           incz &gt; 0 & local_i &gt; 0 ~ \"High-high\",\n           incz &gt; 0 & local_i &lt; 0 ~ \"High-low\",\n           incz &lt; 0 & local_i &gt; 0 ~ \"Low-low\",\n           incz &lt; 0 & local_i &lt; 0 ~ \"Low-high\"\n         ))\n\n#Map\ntm_shape(tracts_sf) + tm_polygons(\"lisa_cluster\", col_alpha = 0)\n\n\n\n\n\n\n\n\nQ2: What is the spatial pattern of clustering in the median household income variable?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "autocorrelation.html#mini-challenge",
    "href": "autocorrelation.html#mini-challenge",
    "title": "13  Spatial Autocorrelation",
    "section": "13.5 Mini Challenge",
    "text": "13.5 Mini Challenge\nUsing the code above as a guide, calculate global and local indicators of spatial autocorrelation for the average commuting time variable and interpret the results.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Spatial Autocorrelation</span>"
    ]
  },
  {
    "objectID": "competencies.html#tier-1-foundations-of-quantitative-methods",
    "href": "competencies.html#tier-1-foundations-of-quantitative-methods",
    "title": "3  Course Competencies",
    "section": "",
    "text": "3.1.1 Assessment Structure:\n\nCompetency: Completes Lab 1-3 with Meets Expectations\nMastery: Completes Case Study #1 with Meets Expectations or a 15 minute oral exam (in office hours)\n\n\n\n3.1.2 Concept 1: The Quantitative Revolution\nGuiding Question: Why did geography become a quantitative discipline, and how did it change how we study space?\n\nDescribe the origins of the Quantitative Revolution in geography and how political and academic pressures pushed geography towards a “spatial science” \nIdentify assumptions built into quantitative reasoning (measurability, objectivity, universality)\nReflect on critiques and limits of the Quantitative Revolution from human and critical geographers\nInterpret early spatial models\nExplain how MAUP, spatial dependence, spatial heterogeneity, and distance decay influence the structure and interpretation of spatial data\nConnect the “spatial problem” to violations of independence in classical inference\n\n\n\n3.1.3 Concept 2: Data and Descriptive Statistics\nGuiding Question: How do we summarize and describe variation in data?\n\nClassify different types of geographic data and variables (e.g., primary vs. secondary, individual vs. aggregated, qualitative vs. quantitative, level of measurement), and explain how these distinctions influence analysis\nExplain and evaluate measurement quality in geographic data, including precision, accuracy, validity, and reliability.\nSummarize and interpret attribute data using descriptive statistics (central tendency, dispersion, shape) and data visualizations\n\n\n\n3.1.4 Concept 3: Spatial Description\nGuiding Question: How do we summarize and describe variation in geographic data?\n\nUse maps to visually analyze spatial patterns and understand the role of classification schemes for grouping spatial data values\nCompute and interpret spatial descriptive statistics (mean center, standard distance, standard deviational ellipse) and explain how they extend non-spatial descriptive measures",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Competencies</span>"
    ]
  },
  {
    "objectID": "competencies.html#tier-2-inferential-statistics",
    "href": "competencies.html#tier-2-inferential-statistics",
    "title": "3  Course Competencies",
    "section": "3.2 Tier 2: Inferential Statistics",
    "text": "3.2 Tier 2: Inferential Statistics\n\n3.2.1 Assessment Structure:\n\nCompetency: Completes Lab 4-6 with Meets Expectations\nMastery: Completes Case Study #2 with Meets Expectations or a 15 minute oral exam (in office hours)\n\n\n\n3.2.2 Concept 4: Probability\nGuiding Question: How can we use probability to quantify uncertainty and understand patterns in events?\n\nCompute and interpret simple event (empirical) probabilities\nCompute and interpret probabilities for discrete and continuous data using theoretical probability distributions\nVisualize probability across space\n\n\n\n3.2.3 Concept 5: Inferential Statistics\nGuiding Question: How can we use sample data to understand and estimate characteristics of a population?\n\nExplain the Central Limit Theorem and why it allows us to generalize from samples to populations.\nIdentify how sampling design affects representativeness and uncertainty (simple, systematic, stratified, cluster, spatial hybrid).\nEstimate and interpret population parameters (means and proportions) from samples (calculate point estimates and confidence intervals)\n\n\n\n3.2.4 Concept 6: Hypothesis Testing\nGuiding Question: How can we generate and test hypotheses about a population using sample data?\n\nDistinguish between the null (H₀) and alternative (H₁) hypotheses\nIdentify the two types of error in hypothesis testing (Type I and Type II)\nFormulate testable hypotheses and identify an appropriate statistical test based on the characteristics of the data \nRun and interpret one sample and two sample t-tests to compare means between one or two groups\nRun and interpret Chi-Square tests to assess relationships between categorical variables\nDifferentiate statistical significance from substantive importance",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Competencies</span>"
    ]
  },
  {
    "objectID": "competencies.html#tier-3-spatial-patterns-and-surfaces",
    "href": "competencies.html#tier-3-spatial-patterns-and-surfaces",
    "title": "3  Course Competencies",
    "section": "3.3 Tier 3: Spatial Patterns and Surfaces",
    "text": "3.3 Tier 3: Spatial Patterns and Surfaces\n\n3.3.1 Assessment Structure:\n\nCompetency: Completes Lab 7-9 with Meets Expectations\nMastery: Completes Case Study #3 with Meets Expectations or a 15 minute oral exam (in office hours)\n\n\n\n3.3.2 Concept 7: Patterns in Events\nGuiding Question: How can we assess the distribution/pattern of individual events in space?\n\nDistinguish random, clustered, and regular event patterns\nCompute and interpret quadrat tests to assess spatial randomness (CSR) and determine statistical significance\nCompute and interpret nearest neighbor statistics to assess spatial randomness and determine statistical significance\nVisualize event patterns using point maps and density surfaces (kernel density estimation)\nEvaluate the effects of scale and edge boundaries on event pattern analysis\n\n\n\n3.3.3 Concept 8: Patterns in Values\nGuiding Question: How can we detect patterns in attribute values across points or areas?\n\nDefine spatial influence (neighborhoods) for point and aerial data using KNN, Queens Case, and distance-based measures\nCompute and interpret measures of global autocorrelation (Moran’s I)\nCompute and interpret measures of local autocorrelation (LISA) and use mapping to identify and analyze spatial patterns (hot-spots, cold-spots)\nDiscuss how scale, zoning, and MAUP influence patterns\n\n\n\n3.3.4 Concept 9: Spatially Continuous Surfaces\nGuiding Question: How can we estimate values at unsampled locations and understand spatial clustering and uncertainty across continuous surfaces?\n\nCompute point estimates at unsampled locations using Thiessen polygons, IDW, and k-point means.\nDefine and interpret the semivariogram to understand spatial clustering, correlation, and range of influence.\nApproximate uncertainty for predictions using sample variance or semivariogram-based approaches and interpret the implications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Course Competencies</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html",
    "href": "hypothesis_testing.html",
    "title": "12  Hypothesis Testing",
    "section": "",
    "text": "12.1 One Sample T-Test\nWe will use the following spatial datasets:\nTo follow along with this tutorial, make a new .Rmd document. As you move through the tutorial add chunks, headers, and relevant text to your document.\nA one-sample t-test is a statistical test used to determine whether the mean of a single sample is significantly different from a known or hypothesized value. It assumes that the data is continuous, that the observations are independent, and that the values are normally distributed in the population.\nTo demonstrate a one sample t-test, we can compare the mean daily PM2.5 value for census tracts across North Carolina to a known value. Although each tract in North Carolina has a mean value (i.e. the census tracts are not sampled), we can consider this a “natural sample”.\nIn geography, inferential statistics are sometimes applied even when datasets include all observed units, because many spatial phenomena contain both systematic and random components. These datasets can be treated as “natural samples”, where the observed variation reflects real-world randomness. For example, mean daily PM2.5 values across all census tracts in a state are influenced by systematic factors like industrial activity and traffic patterns, but also by unpredictable environmental processes such as local weather conditions. This natural variability allows the observed values to be treated as a sample from the broader population of possible PM2.5 conditions, making inferential tests like the one-sample t-test appropriate.\nThe annual average PM2.5 value for the eastern United States is 7.8µg/m3 . Therefore, we could use a one-sample t-test to see if the North Carolina average differs from the United States average.\nBefore we run a statistical test, the first thing we have to do is check our assumptions. We know that PM2.5 is a continuous variable. Because this data is spatial, it likely violates the assumption of independence (but we will put a pin in that for now and assume independence) We can estimate the distribution of the population by using the distribution of the sample. A QQ-Plot indicates that the data distribution is non-normal.\nqqnorm(pm$meanpm_2016_2020)\nqqline(pm$meanpm_2016_2020, col = \"steelblue\", lwd = 2)\nAt this point, we have two options for testing whether our sample mean differs from a hypothesized value. The first option is to use a non-parametric test, which does not assume any particular distribution for the population. A non-parametric alternative for a one-sample t-test is the Wilcoxon Signed-Rank Test. The second option is to still use the one-sample t-test. While this test assumes that the population is normally distributed, the Central Limit Theorem tells us that, for large sample sizes, the sampling distribution of the mean is approximately normal regardless of the population distribution. Since our sample is large (n = 2672), the t-test is valid in this case, even if the data are not normal.\nSince this class focuses on parametric tests, we will continue on with our t-test. The mean of our dataset is\nmean(pm$meanpm_2016_2020)\n\n[1] 7.546838\nQ1: Formulate a null hypothesis that would use a one sample, one-sided t-test\nWe can then test our hypothesis\n#test mean \nt.test(pm$meanpm_2016_2020, mu = 7.8, alternative = \"less\", conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  pm$meanpm_2016_2020\nt = -12.94, df = 2671, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is less than 7.8\n95 percent confidence interval:\n     -Inf 7.579029\nsample estimates:\nmean of x \n 7.546838\nIn this case, the low p-value of our t.test allows us to reject the null hypothesis in favor of the alternative hypothesis:\nH1: The mean PM2.5 value for North Carolina is less than the mean value for the eastern US.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#two-sample-t-test",
    "href": "hypothesis_testing.html#two-sample-t-test",
    "title": "12  Hypothesis Testing",
    "section": "12.2 Two Sample T-Test",
    "text": "12.2 Two Sample T-Test\nA two-sample t-test compares the means of two different samples. For instance, we could compare the mean PM2.5 in urban vs. rural census tracts.\nThe assumptions for the two sample t-test are the same as the one-sample t-test with one addition: equal variance (also called homoscedasticity). This means that the two groups being compared should have similar variability in their values. If the variances are very different, the standard two-sample t-test may not be appropriate. For the purposes of this analysis, we will assume that the variances of PM2.5 values in urban and rural census tracts are equal. This assumption allows us to use the standard two-sample t-test. However, if the variances were substantially different, we would need to use a version of the test that does not assume equal variance (Welch’s t-test).\nQ2. Formulate a null hypothesis that would use a two sample two-sided t-test.\n\n#convert data to factor\npm$rural &lt;- factor(pm$rural, levels = c(0, 1), labels = c(\"urban\", \"rural\"))\n\n# Then run a standard two-sample t-test\nt.test(meanpm_2016_2020 ~ rural, data = pm, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  meanpm_2016_2020 by rural\nt = 20.389, df = 2670, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group urban and group rural is not equal to 0\n95 percent confidence interval:\n 0.7590222 0.9205533\nsample estimates:\nmean in group urban mean in group rural \n           7.770299            6.930511 \n\n\nBased on our t-test results, we can reject the null hypothesis in favor of the alternative hypothesis:\nH1: The mean PM2.5 in urban and rural census tracts is different.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#paired-t-test",
    "href": "hypothesis_testing.html#paired-t-test",
    "title": "12  Hypothesis Testing",
    "section": "12.3 Paired T-Test",
    "text": "12.3 Paired T-Test\nA paired t-test evaluates whether the average change between two paired observations is significantly different from zero. It is typically used when the same individuals or locations are measured at two time points or under two conditions. For instance, we could consider our summer temperature values. In this dataset, we have a sample of 480 locations and two different values– mean summer temperature from 2000-2010 and mean summer temperature from 2010-2020.\nWe can consider this dataset a true simple random sample.\nQ3: Formulate a null hypothesis that would use a one-sample paired t-test\n\nt.test(summer_temp$av_summer_2010_2020, summer_temp$av_summer_2000_2010, paired = TRUE, alternative = \"greater\", conf.level = 0.95)\n\n\n    Paired t-test\n\ndata:  summer_temp$av_summer_2010_2020 and summer_temp$av_summer_2000_2010\nt = 192.75, df = 479, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 1.139164      Inf\nsample estimates:\nmean difference \n       1.148988 \n\n\nBased on our t-test results, we can reject the null hypothesis in favor of the alternative hypothesis:\nH1: The mean summer temperature in 2010-2020 is higher than the mean summer temperature in 2000-2010.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#chi-square-test-of-independence",
    "href": "hypothesis_testing.html#chi-square-test-of-independence",
    "title": "12  Hypothesis Testing",
    "section": "12.4 Chi Square Test of Independence",
    "text": "12.4 Chi Square Test of Independence\nThe Chi-square test of independence is a statistical test used to determine whether two categorical variables are associated with one another. Because the variables are categorical rather than continuous, the test is non-parametric, meaning it does not assume any particular underlying distribution. Our American Community Data includes three categorical variables:\n\nRural/ Non-rural\nLow income/ High Income\nLow commute / High commute\n\nWe could examine whether any pair of these variables is related. For instance, whether rural/non-rural and low commute/ high commute is related. In this case, the null hypothesis would be:\nH0: Rural/non-rural status and commute category are independent (there is no association between them).\nWe can start by looking at a contingency table.\n\ntable(acs$rural, acs$high_commute)\n\n           \n            high commute low commute\n  non-rural          310        1620\n  rural              118         580\n\n\n\nchisq.test(table(acs$rural, acs$high_commute))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(acs$rural, acs$high_commute)\nX-squared = 0.20909, df = 1, p-value = 0.6475\n\n\nIn this case, our X-squared value is low (.209) and the p-value is high (.65), meaning that we fail to reject the null hypothesis.\nQ4: Run a chi square test of independence for another set of variables",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis_testing.html#mini-challenge",
    "href": "hypothesis_testing.html#mini-challenge",
    "title": "12  Hypothesis Testing",
    "section": "12.5 Mini Challenge",
    "text": "12.5 Mini Challenge\nThis challenge will ask you to run hypothesis tests on mobility data for North Carolina counties during Covid-19. We will consider the county values as a natural sample for North Carolina. The mobility dataset includes the following variables:\n\nrural - rural/nonrural flag for each county\nworkplace_change- percent change in workplace mobility from March-May 2020\nworkplace_change_may_dec - percent change in workplace mobility by county from May 2020- Dec 2020\n\n\nCreate two maps– one for each mobility variable. Do you notice any spatial patterns?\nCalculate the mean workplace mobility for the state of NC during the two time periods. Develop a null hypothesis and run a paired t-test between the two time periods\nCalculate the mean workplace mobility for rural counties and urban counties. Develop a null hypothesis and run an independent samples t-test between the two geographical regions.\nConsider that the known average mobility during the March-May period across the US was -32. Develop a null hypothesis and run a one sample t-test between the known value and the sample value.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "labs.html#lab-2",
    "href": "labs.html#lab-2",
    "title": "14  Labs",
    "section": "",
    "text": "14.1.1 Overview\nIn the Describing Data tutorial we worked with spatial datasets describing the rurality of North Carolina census tracts, summer temperatures across the United States, and a demographic variable measured at the census tract level in North Carolina. Spatial data is often aggregated to specific geographic units (for instance, to protect individual privacy or simplify a complex dataset). This aggregation influences the descriptive statistics we calculate.\nIn this lab, you will examine demographic and climate data at a different level of spatial aggregation and observe how the descriptive statistics change as a result (remember the underlying data values are not changing, only the geographic units used to summarize them). You will also explore an alternative way of defining “rural” and compare how this definition shapes your interpretation of the data.\nRemember that you can use the Command Compendium to help you modify R commands\n\n\n14.1.2 Specifications\nThis lab is designed to assess the Concept 2 Competencies. You’ll be evaluated on the following specifications:\n\n\n\n\n\n\nSpecification\n\n\n\n\nStudent submits HTML and RMD versions the R Markdown file.\n\n\nThe RMD is clearly organized (appropriate headings, code chunk formatting, and clean output). Minor formatting issues don’t prevent the work from being read and understood.\n\n\nDescribing Rurality: Student produces a descriptive statistics table, a bar chart, and a map. Written interpretation demonstrates understanding of the patterns and differences between NC Rural Center and USDA RUCA definitions\n\n\nDescribing Climate: Student produces descriptive statistics, two visualizations, and a map. Written interpretations demonstrate understanding of central tendency, dispersion, spatial patterns, and aggregation effects\n\n\nDescribing Demographics: Student produces descriptive statistics, two visualizations, and a map. Written interpretation demonstrates understanding of central tendency, dispersion, spatial patterns, and differences across aggregation levels.\n\n\n\n\n\n14.1.3 Lab Instructions\n\nCreate a new .Rmd named “LASTNAME_lab2.Rmd”. Save it into your GEOG391 folder.\nRemove sample text (leaving the header and set-up chunk). Add a chunk for loading libraries and reading in data. Add the following code into that chunk:\n\n## load necessary libraries\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(sf)\nlibrary(e1071)\nlibrary(tmap)\n\n\n#read in data\nrurality &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1aVDPgiZLUnKGb5k2SQo7ScDO2WNvX3Nd\")\ncounty_climate_normals &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1sf0cmMaeHGra7AQheYAjTiC3wGCzB4FG\")\nacs_county_nc &lt;- st_read(\"https://drive.google.com/uc?export=download&id=1Cv9iUFFi2RPvDBiuYGolY7dDuUR7_raN\")\n\nAdd “Describing Rurality” as a third-level header. In this section you will compute descriptive statistics and visualizations for North Carolina census tracts using the North Carolina Rural Center’s definition of rurality (rural = fewer than 250 people per square mile). The NC Rural Center variable is named nc_rural_center in the dataset. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of rurality by census tract\nCreate a bar chart of rurality by census tract\nCreate a map of rurality by census tract\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the bar chart and descriptive statistics tell us about the data\nDescribe what the map tells us about the spatial pattern of the data\nCompare the NC Rural Center definition of rural with the USDA RUCA codes, describing how the descriptive statistics differ and how the spatial patterns differ across the state\n\nAdd “Describing Climate” as a third-level header. In this section you will compute descriptive statistics and visualizations for average summer temperatures per county in the U.S. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of average summer temperature by U.S county\nCreate two appropriate non-map data visualizations of average summer temperature by U.S. county\nCreate a map of average of summer temperature by U.S. county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about average summer temperature (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to latitude/longitude (point-level) data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data\n\n\nAdd “Describing Demographics” as a third-level header. In this section you will compute descriptive statistics and visualizations for your selected variable in Mini Challenge #1. Add a code chunk below your header. Your code chunk should:\n\nCreate a descriptive statistics table of your variable by NC county\nCreate two appropriate non-map data visualizations of your variable by NC county\nCreate a map of your variable by NC county\n\nBelow your chunk, you should write 1-2 paragraphs that do the following:\n\nDescribe what the descriptive statistics table and data visualizations tell us about your variable (make sure to discuss central tendency, dispersion, shape)\nDescribe what the map tells us about the spatial pattern of the data\nCompare the county-level data results to the census tract data, noting\n\nDifferences in descriptive statistics, visualizations, and spatial patterns\nHow these differences may relate to the scale or aggregation of the data\n\n\nKnit your .Rmd and make sure the formatting and organization are clear in the knitted .html document",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Labs</span>"
    ]
  },
  {
    "objectID": "probability.html#section",
    "href": "probability.html#section",
    "title": "9  Probability and Probability Distributions",
    "section": "9.7 ",
    "text": "9.7",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Probability and Probability Distributions</span>"
    ]
  }
]